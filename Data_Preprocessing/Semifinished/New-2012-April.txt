###############################################################
XO Outages
###############################################################
  Anyone experiencing any XO Outages?  In the Philadelphia area our lines are
 straight to busy.
 -------------- next part --------------
 An HTML attachment was scrubbed...
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120402/35d08f71/attachment.html>
 ----- Original Message -----
 We have some direct PRIs from XO in Tampa FL, and I have no reports from the
 floor of circuit problems at this time.
 Cheers,
 -- jra
 -- 
 Jay R. Ashworth                  Baylink                       jra at baylink.com
 Designer                     The Things I Think                       RFC 2100
 Ashworth & Associates     http://baylink.pitas.com         2000 Land Rover DII
 St Petersburg FL USA      http://photo.imageinc.us             +1 727 647 1274
###############################################################
END
###############################################################

###############################################################
West Coast USA Charter Issues?
###############################################################
 Seeing weird issues as we did 2 weeks ago...anyone else?  This time, it appears to make it through Cogent's network but dies in Charters (unlike 2 weeks ago when traceroutes died in Cogent's network).
 $ traceroute 24.205.254.116
 traceroute to 24.205.254.116 (24.205.254.116), 30 hops max, 60 byte packets
 1  72.29.165.193 (72.29.165.193)  3.071 ms  3.113 ms  3.119 ms
 2  gae-2-2.core01.dwni.pin.sbp1.digitalwest.net (72.29.160.229)  29.396 ms  29.396 ms  29.388 ms
 3  ge-1-2-0-10.core04.dwni.pin.sbp1.digitalwest.net (72.29.160.193)  0.880 ms  0.929 ms  0.978 ms
 4  gae-0-8.core02.dwni.pin.sjc1.digitalwest.net (72.29.160.186)  3.904 ms  3.947 ms  3.937 ms
 5  gi3-9.mag01.sjc03.atlas.cogentco.com (38.104.138.29)  194.431 ms  194.477 ms  194.500 ms
 6  te7-4.mpd01.sjc03.atlas.cogentco.com (154.54.82.89)  14.665 ms  13.881 ms  13.905 ms
 7  customer-154-138.demarc.cogentco.com (38.104.138.154)  7.968 ms * *
 8  bbr01snjsca-tge-0-0-0-2.snjs.ca.charter.com (96.34.0.2)  4.768 ms * *
 9  bbr02snloca-tge-0-1-0-2.snlo.ca.charter.com (96.34.0.77)  11.857 ms  11.905 ms *
 10  dtr04snloca-tge-0-1-0-4.snlo.ca.charter.com (96.34.2.3)  7.991 ms  8.039 ms  8.128 ms
 11  cts01snloca-tge-1-0-0.snlo.ca.charter.com (96.34.120.75)  7.884 ms  7.862 ms *
 12  * * *
 13  * * *
 14  * * *
 15  * * *
 16  * * *
 17  * * *
 18  * * *
 19  * * *
 20  * * *
 21  * * *
 22  * * *
 23  * * *
 24  * * *
 25  * * *
 26  * * *
 27  * * *
 28  * * *
 29  * * *
 30  * * *
 $
 $ traceroute 68.189.122.61
 traceroute to 68.189.122.61 (68.189.122.61), 30 hops max, 60 byte packets
 1  72.29.165.193 (72.29.165.193)  3.906 ms  3.884 ms  3.867 ms
 2  gae-2-2.core01.dwni.pin.sbp1.digitalwest.net (72.29.160.229)  0.314 ms  0.329 ms  0.319 ms
 3  ge-0-0-0-12.core04.dwni.pin.sbp1.digitalwest.net (72.29.160.197)  0.863 ms  0.893 ms  0.941 ms
 4  gae-0-8.core02.dwni.pin.sjc1.digitalwest.net (72.29.160.186)  3.917 ms  3.917 ms  3.910 ms
 5  gi3-9.mag01.sjc03.atlas.cogentco.com (38.104.138.29)  4.025 ms  4.092 ms  4.109 ms
 6  te7-4.mpd01.sjc03.atlas.cogentco.com (154.54.82.89)  117.823 ms  117.682 ms  117.692 ms
 7  * customer-154-138.demarc.cogentco.com (38.104.138.154)  5.132 ms *
 8  bbr01snjsca-tge-0-0-0-2.snjs.ca.charter.com (96.34.0.2)  7.387 ms  5.197 ms  5.190 ms
 9  bbr02snloca-tge-0-1-0-6.snlo.ca.charter.com (96.34.0.128)  11.625 ms  9.941 ms *
 10  * * dtr04snloca-tge-0-1-0-5.snlo.ca.charter.com (96.34.2.81)  7.877 ms
 11  * * dtr03snloca-tge-0-2-0-1.snlo.ca.charter.com (96.34.120.179)  8.156 ms
 12  dtr01snloca-vln-99.snlo.ca.charter.com (96.34.120.20)  8.224 ms * dtr01snloca-tge-4-3.snlo.ca.charter.com (96.34.120.111)  7.967 ms
 13  acr01atscca-tge-4-1.atsc.ca.charter.com (96.34.120.151)  8.422 ms  8.348 ms  8.452 ms
 14  cts02atscca-gbe-1-0-1.atsc.ca.charter.com (96.34.120.11)  8.269 ms * *
 15  * * *
 16  * * *
 17  * * *
 18  * * *
 19  * * *
 20  * * *
 21  * * *
 22  * * *
 23  * * *
 24  * * *
 25  * * *
 26  * * *
 27  * * *
 28  * * *
 29  * * *
 30  * * *
 $
 Regards,
 Kirk Coviello | VP, Support Services
 +1.805.781.9378 | +1.888.781.WEST
 [Description: Description: Description: Description: Description: DWLogo]<http://www.digitalwest.net/>
 -------------- next part --------------
 An HTML attachment was scrubbed...
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120404/2ab686cb/attachment.html>
 -------------- next part --------------
 A non-text attachment was scrubbed...
 Name: image001.jpg
 Type: image/jpeg
 Size: 2419 bytes
 Desc: image001.jpg
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120404/2ab686cb/attachment.jpg>
 And even more bizarre, traceroute to 24.205.254.116 succeeds from my desktop, but not my linux box:
 Public source IP for my desktop is 72.29.166.5, public IP for the linux box (see traces in first email) is 72.29.165.196.
 H:\>tracert 24.205.254.116
 Tracing route to 24-205-254-116.dhcp.mrba.ca.charter.com [24.205.254.116]
 over a maximum of 30 hops:
   1    <1 ms    <1 ms    <1 ms
   2     2 ms     1 ms     2 ms  vlanxxx.aggr1.dwni.net [72.29.166.1]
   3    15 ms     1 ms    <1 ms  gae-2-2.core01.dwni.pin.sbp1.digitalwest.net [72
 .29.160.229]
   4     1 ms     1 ms     1 ms  ge-1-2-0-10.core04.dwni.pin.sbp1.digitalwest.net
 [72.29.160.193]
   5     4 ms     4 ms     4 ms  gae-0-8.core02.dwni.pin.sjc1.digitalwest.net [72
 .29.160.186]
   6     4 ms     4 ms     4 ms  gi3-9.mag01.sjc03.atlas.cogentco.com [38.104.138
 .29]
   7     4 ms     4 ms     4 ms  te7-4.mpd01.sjc03.atlas.cogentco.com [154.54.82.
 89]
   8     6 ms     6 ms     7 ms  38.104.138.158
   9     9 ms     6 ms     7 ms  bbr01snjsca-tge-0-2-0-2.snjs.ca.charter.com [96.
 34.0.200]
 10    10 ms    11 ms    11 ms  bbr02snloca-tge-0-2-0-4.snlo.ca.charter.com [96.
 34.0.172]
 11     8 ms     8 ms     8 ms  dtr04snloca-tge-0-1-0-4.snlo.ca.charter.com [96.
 34.2.3]
 12     8 ms     8 ms     8 ms  cts01snloca-tge-1-0-0.snlo.ca.charter.com [96.34
 .120.75]
 13    15 ms    19 ms    17 ms  24-205-254-116.dhcp.mrba.ca.charter.com [24.205.
 254.116]
 Trace complete.
 Regards,
 Kirk Coviello | VP, Support Services
 +1.805.781.9378 | +1.888.781.WEST
 [Description: Description: Description: Description: Description: DWLogo]<http://www.digitalwest.net/>
 From: outages-bounces at outages.org [mailto:outages-bounces at outages.org] On Behalf Of Kirk Coviello
 Sent: Wednesday, April 04, 2012 3:47 PM
 Seeing weird issues as we did 2 weeks ago...anyone else?  This time, it appears to make it through Cogent's network but dies in Charters (unlike 2 weeks ago when traceroutes died in Cogent's network).
 $ traceroute 24.205.254.116
 traceroute to 24.205.254.116 (24.205.254.116), 30 hops max, 60 byte packets
 1  72.29.165.193 (72.29.165.193)  3.071 ms  3.113 ms  3.119 ms
 2  gae-2-2.core01.dwni.pin.sbp1.digitalwest.net (72.29.160.229)  29.396 ms  29.396 ms  29.388 ms
 3  ge-1-2-0-10.core04.dwni.pin.sbp1.digitalwest.net (72.29.160.193)  0.880 ms  0.929 ms  0.978 ms
 4  gae-0-8.core02.dwni.pin.sjc1.digitalwest.net (72.29.160.186)  3.904 ms  3.947 ms  3.937 ms
 5  gi3-9.mag01.sjc03.atlas.cogentco.com (38.104.138.29)  194.431 ms  194.477 ms  194.500 ms
 6  te7-4.mpd01.sjc03.atlas.cogentco.com (154.54.82.89)  14.665 ms  13.881 ms  13.905 ms
 7  customer-154-138.demarc.cogentco.com (38.104.138.154)  7.968 ms * *
 8  bbr01snjsca-tge-0-0-0-2.snjs.ca.charter.com (96.34.0.2)  4.768 ms * *
 9  bbr02snloca-tge-0-1-0-2.snlo.ca.charter.com (96.34.0.77)  11.857 ms  11.905 ms *
 10  dtr04snloca-tge-0-1-0-4.snlo.ca.charter.com (96.34.2.3)  7.991 ms  8.039 ms  8.128 ms
 11  cts01snloca-tge-1-0-0.snlo.ca.charter.com (96.34.120.75)  7.884 ms  7.862 ms *
 12  * * *
 13  * * *
 14  * * *
 15  * * *
 16  * * *
 17  * * *
 18  * * *
 19  * * *
 20  * * *
 21  * * *
 22  * * *
 23  * * *
 24  * * *
 25  * * *
 26  * * *
 27  * * *
 28  * * *
 29  * * *
 30  * * *
 $
 $ traceroute 68.189.122.61
 traceroute to 68.189.122.61 (68.189.122.61), 30 hops max, 60 byte packets
 1  72.29.165.193 (72.29.165.193)  3.906 ms  3.884 ms  3.867 ms
 2  gae-2-2.core01.dwni.pin.sbp1.digitalwest.net (72.29.160.229)  0.314 ms  0.329 ms  0.319 ms
 3  ge-0-0-0-12.core04.dwni.pin.sbp1.digitalwest.net (72.29.160.197)  0.863 ms  0.893 ms  0.941 ms
 4  gae-0-8.core02.dwni.pin.sjc1.digitalwest.net (72.29.160.186)  3.917 ms  3.917 ms  3.910 ms
 5  gi3-9.mag01.sjc03.atlas.cogentco.com (38.104.138.29)  4.025 ms  4.092 ms  4.109 ms
 6  te7-4.mpd01.sjc03.atlas.cogentco.com (154.54.82.89)  117.823 ms  117.682 ms  117.692 ms
 7  * customer-154-138.demarc.cogentco.com (38.104.138.154)  5.132 ms *
 8  bbr01snjsca-tge-0-0-0-2.snjs.ca.charter.com (96.34.0.2)  7.387 ms  5.197 ms  5.190 ms
 9  bbr02snloca-tge-0-1-0-6.snlo.ca.charter.com (96.34.0.128)  11.625 ms  9.941 ms *
 10  * * dtr04snloca-tge-0-1-0-5.snlo.ca.charter.com (96.34.2.81)  7.877 ms
 11  * * dtr03snloca-tge-0-2-0-1.snlo.ca.charter.com (96.34.120.179)  8.156 ms
 12  dtr01snloca-vln-99.snlo.ca.charter.com (96.34.120.20)  8.224 ms * dtr01snloca-tge-4-3.snlo.ca.charter.com (96.34.120.111)  7.967 ms
 13  acr01atscca-tge-4-1.atsc.ca.charter.com (96.34.120.151)  8.422 ms  8.348 ms  8.452 ms
 14  cts02atscca-gbe-1-0-1.atsc.ca.charter.com (96.34.120.11)  8.269 ms * *
 15  * * *
 16  * * *
 17  * * *
 18  * * *
 19  * * *
 20  * * *
 21  * * *
 22  * * *
 23  * * *
 24  * * *
 25  * * *
 26  * * *
 27  * * *
 28  * * *
 29  * * *
 30  * * *
 $
 Regards,
 Kirk Coviello | VP, Support Services
 +1.805.781.9378 | +1.888.781.WEST
 [Description: Description: Description: Description: Description: DWLogo]<http://www.digitalwest.net/>
 -------------- next part --------------
 An HTML attachment was scrubbed...
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120404/6f148136/attachment-0001.html>
 -------------- next part --------------
 A non-text attachment was scrubbed...
 Name: image001.jpg
 Type: image/jpeg
 Size: 2419 bytes
 Desc: image001.jpg
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120404/6f148136/attachment-0001.jpg>
  <2B43424F66F03F4482C2CBB1E691656E27E3D187@exch-be.sbp1.ad.digitalwest.lan>
 Cleared up now, and Charter stating that they fixed the issue.  Total time of about 2 hours.  I'll follow up if
 a) I get any further information and
 b) anyone is interested.
 Regards,
 Kirk Coviello | VP, Support Services
 +1.805.781.9378 | +1.888.781.WEST
 [Description: Description: Description: Description: Description: DWLogo]<http://www.digitalwest.net/>
 From: outages-bounces at outages.org [mailto:outages-bounces at outages.org] On Behalf Of Kirk Coviello
 Sent: Wednesday, April 04, 2012 3:53 PM
 And even more bizarre, traceroute to 24.205.254.116 succeeds from my desktop, but not my linux box:
 Public source IP for my desktop is 72.29.166.5, public IP for the linux box (see traces in first email) is 72.29.165.196.
 H:\>tracert 24.205.254.116
 Tracing route to 24-205-254-116.dhcp.mrba.ca.charter.com [24.205.254.116]
 over a maximum of 30 hops:
   1    <1 ms    <1 ms    <1 ms
   2     2 ms     1 ms     2 ms  vlanxxx.aggr1.dwni.net [72.29.166.1]
   3    15 ms     1 ms    <1 ms  gae-2-2.core01.dwni.pin.sbp1.digitalwest.net [72
 .29.160.229]
   4     1 ms     1 ms     1 ms  ge-1-2-0-10.core04.dwni.pin.sbp1.digitalwest.net
 [72.29.160.193]
   5     4 ms     4 ms     4 ms  gae-0-8.core02.dwni.pin.sjc1.digitalwest.net [72
 .29.160.186]
   6     4 ms     4 ms     4 ms  gi3-9.mag01.sjc03.atlas.cogentco.com [38.104.138
 .29]
   7     4 ms     4 ms     4 ms  te7-4.mpd01.sjc03.atlas.cogentco.com [154.54.82.
 89]
   8     6 ms     6 ms     7 ms  38.104.138.158
   9     9 ms     6 ms     7 ms  bbr01snjsca-tge-0-2-0-2.snjs.ca.charter.com [96.
 34.0.200]
 10    10 ms    11 ms    11 ms  bbr02snloca-tge-0-2-0-4.snlo.ca.charter.com [96.
 34.0.172]
 11     8 ms     8 ms     8 ms  dtr04snloca-tge-0-1-0-4.snlo.ca.charter.com [96.
 34.2.3]
 12     8 ms     8 ms     8 ms  cts01snloca-tge-1-0-0.snlo.ca.charter.com [96.34
 .120.75]
 13    15 ms    19 ms    17 ms  24-205-254-116.dhcp.mrba.ca.charter.com [24.205.
 254.116]
 Trace complete.
 Regards,
 Kirk Coviello | VP, Support Services
 +1.805.781.9378 | +1.888.781.WEST
 [Description: Description: Description: Description: Description: DWLogo]<http://www.digitalwest.net/>
 From: outages-bounces at outages.org<mailto:outages-bounces at outages.org> [mailto:outages-bounces at outages.org]<mailto:[mailto:outages-bounces at outages.org]> On Behalf Of Kirk Coviello
 Sent: Wednesday, April 04, 2012 3:47 PM
 Seeing weird issues as we did 2 weeks ago...anyone else?  This time, it appears to make it through Cogent's network but dies in Charters (unlike 2 weeks ago when traceroutes died in Cogent's network).
 $ traceroute 24.205.254.116
 traceroute to 24.205.254.116 (24.205.254.116), 30 hops max, 60 byte packets
 1  72.29.165.193 (72.29.165.193)  3.071 ms  3.113 ms  3.119 ms
 2  gae-2-2.core01.dwni.pin.sbp1.digitalwest.net (72.29.160.229)  29.396 ms  29.396 ms  29.388 ms
 3  ge-1-2-0-10.core04.dwni.pin.sbp1.digitalwest.net (72.29.160.193)  0.880 ms  0.929 ms  0.978 ms
 4  gae-0-8.core02.dwni.pin.sjc1.digitalwest.net (72.29.160.186)  3.904 ms  3.947 ms  3.937 ms
 5  gi3-9.mag01.sjc03.atlas.cogentco.com (38.104.138.29)  194.431 ms  194.477 ms  194.500 ms
 6  te7-4.mpd01.sjc03.atlas.cogentco.com (154.54.82.89)  14.665 ms  13.881 ms  13.905 ms
 7  customer-154-138.demarc.cogentco.com (38.104.138.154)  7.968 ms * *
 8  bbr01snjsca-tge-0-0-0-2.snjs.ca.charter.com (96.34.0.2)  4.768 ms * *
 9  bbr02snloca-tge-0-1-0-2.snlo.ca.charter.com (96.34.0.77)  11.857 ms  11.905 ms *
 10  dtr04snloca-tge-0-1-0-4.snlo.ca.charter.com (96.34.2.3)  7.991 ms  8.039 ms  8.128 ms
 11  cts01snloca-tge-1-0-0.snlo.ca.charter.com (96.34.120.75)  7.884 ms  7.862 ms *
 12  * * *
 13  * * *
 14  * * *
 15  * * *
 16  * * *
 17  * * *
 18  * * *
 19  * * *
 20  * * *
 21  * * *
 22  * * *
 23  * * *
 24  * * *
 25  * * *
 26  * * *
 27  * * *
 28  * * *
 29  * * *
 30  * * *
 $
 $ traceroute 68.189.122.61
 traceroute to 68.189.122.61 (68.189.122.61), 30 hops max, 60 byte packets
 1  72.29.165.193 (72.29.165.193)  3.906 ms  3.884 ms  3.867 ms
 2  gae-2-2.core01.dwni.pin.sbp1.digitalwest.net (72.29.160.229)  0.314 ms  0.329 ms  0.319 ms
 3  ge-0-0-0-12.core04.dwni.pin.sbp1.digitalwest.net (72.29.160.197)  0.863 ms  0.893 ms  0.941 ms
 4  gae-0-8.core02.dwni.pin.sjc1.digitalwest.net (72.29.160.186)  3.917 ms  3.917 ms  3.910 ms
 5  gi3-9.mag01.sjc03.atlas.cogentco.com (38.104.138.29)  4.025 ms  4.092 ms  4.109 ms
 6  te7-4.mpd01.sjc03.atlas.cogentco.com (154.54.82.89)  117.823 ms  117.682 ms  117.692 ms
 7  * customer-154-138.demarc.cogentco.com (38.104.138.154)  5.132 ms *
 8  bbr01snjsca-tge-0-0-0-2.snjs.ca.charter.com (96.34.0.2)  7.387 ms  5.197 ms  5.190 ms
 9  bbr02snloca-tge-0-1-0-6.snlo.ca.charter.com (96.34.0.128)  11.625 ms  9.941 ms *
 10  * * dtr04snloca-tge-0-1-0-5.snlo.ca.charter.com (96.34.2.81)  7.877 ms
 11  * * dtr03snloca-tge-0-2-0-1.snlo.ca.charter.com (96.34.120.179)  8.156 ms
 12  dtr01snloca-vln-99.snlo.ca.charter.com (96.34.120.20)  8.224 ms * dtr01snloca-tge-4-3.snlo.ca.charter.com (96.34.120.111)  7.967 ms
 13  acr01atscca-tge-4-1.atsc.ca.charter.com (96.34.120.151)  8.422 ms  8.348 ms  8.452 ms
 14  cts02atscca-gbe-1-0-1.atsc.ca.charter.com (96.34.120.11)  8.269 ms * *
 15  * * *
 16  * * *
 17  * * *
 18  * * *
 19  * * *
 20  * * *
 21  * * *
 22  * * *
 23  * * *
 24  * * *
 25  * * *
 26  * * *
 27  * * *
 28  * * *
 29  * * *
 30  * * *
 $
 Regards,
 Kirk Coviello | VP, Support Services
 +1.805.781.9378 | +1.888.781.WEST
 [Description: Description: Description: Description: Description: DWLogo]<http://www.digitalwest.net/>
 -------------- next part --------------
 An HTML attachment was scrubbed...
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120404/d91f7330/attachment.html>
 -------------- next part --------------
 A non-text attachment was scrubbed...
 Name: image001.jpg
 Type: image/jpeg
 Size: 2419 bytes
 Desc: image001.jpg
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120404/d91f7330/attachment.jpg>
###############################################################
END
###############################################################

###############################################################
Att BVOIP Outage
###############################################################
 As of about 9am CST we started getting duplicate ISDN sessions for all of
 our inbound calls on an IP FLEX circuit from ATT, outbound calls are fine,
 but inbound connect and give an error "Caller is unreachable at this time".
 Speaking with ATT, they are aware of the issue and have reports of issues
 in the mid west (we are in Illinois). No master ticket and no ETTR as of
 yet, but supposedly they are working on the issue. Just a heads up.
 Jeff
 -------------- next part --------------
 An HTML attachment was scrubbed...
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120405/105289ab/attachment-0001.html>
###############################################################
END
###############################################################

###############################################################
CenturyLink Washington
###############################################################
 CenturyLink is experiencing a significant Ethernet outage since
 approximately 0100 PST today.  Several of my circuits are affected. No
 ETA, master ticket #5660434.  I understand it's a hardware issue in
 Gig Harbor WA
 Cheers
 -Chris
###############################################################
END
###############################################################

###############################################################
Verizon Wireless issues in Hudson Valley, NY?
###############################################################
 Anyone else experiencing problems with Verizon Wireless services in
 the Hudson Valley of NY?  I have a significant number of users in
 Ulster County reporting no voice or data services.
 Thanks,
 -cjp
###############################################################
END
###############################################################

###############################################################
AWS Outages?
###############################################################
 Anyone seeing issues with Amazon Web Services right now?  (04-05-12
 18-00 pacific)
 Thanks!
 Neil
 No. What availability zone?
 On Apr 5, 2012, at 6:00 PM, Neil Ticktin <neil-lists at xplain.com> wrote:
 Our customers who also maintain sites/backup to AWS seem fine.
 Sincerely,
 William Kern
 PixelGate Networks
 On 4/5/12 6:00 PM, Neil Ticktin wrote:
 On 4/5/2012 9:00 PM, Neil Ticktin wrote:
 A cursory check shows that our instances in US-EAST-1C and D are
 responding normally.
 -- Ben
 Everything seems fine here:
 http://status.aws.amazon.com/
 Click the "Report an issue" to report any issues you're seeing.
 Frank
 -----Original Message-----
 From: outages-bounces at outages.org [mailto:outages-bounces at outages.org] On
 Behalf Of Neil Ticktin
 Sent: Thursday, April 05, 2012 8:01 PM
 Anyone seeing issues with Amazon Web Services right now?  (04-05-12
 18-00 pacific)
 Thanks!
 Neil
 _______________________________________________
 Outages mailing list
 Outages at outages.org
 https://puck.nether.net/mailman/listinfo/outages
###############################################################
END
###############################################################

###############################################################
Significant packet loss over Level3 in Dallas
###############################################################
 Anyone else seeing packet loss on routes going through
 *.edge4.dallas3.level3.net? We just noticed it in the past 30m; not sure
 how long it's been going on.
 Just for reference, I'm directly connected to edge9 in Dallas and have not seen anything or heard any reports from our clients. I'm checking one destination which (based on PTR records) appears to traverse edge4 and am not currently seeing any loss to the destination.
 -Vinny
 -----Original Message-----
 From: outages-bounces at outages.org [mailto:outages-bounces at outages.org] On Behalf Of Steve Dispensa
 Sent: Friday, April 06, 2012 1:30 PM
 Anyone else seeing packet loss on routes going through
 *.edge4.dallas3.level3.net? We just noticed it in the past 30m; not sure
 how long it's been going on.
 _______________________________________________
 Outages mailing list
 Outages at outages.org
 https://puck.nether.net/mailman/listinfo/outages
  <FD9B2CB2B33E394FAE3B74669547605712B73CFD@DFWX10HMPTC01.AMER.DELL.COM>
 Problem seems to have resolved itself as of about 3 minutes ago.
  -Steve
 not
 am
 On
 sure
 On 04/06/12?12:29?-0500, Steve Dispensa wrote:
 >Anyone else seeing packet loss on routes going through
 >*.edge4.dallas3.level3.net? We just noticed it in the past 30m; not sure
 >how long it's been going on.
 I am not seeing any packet loss as of Fri, 06 Apr 2012 13:32:02 -0500:
 dwhite at quark:~$ traceroute slashdot.org
 traceroute to slashdot.org (216.34.181.45), 30 hops max, 60 byte packets
   1  10.0.2.1 (10.0.2.1)  0.885 ms  1.032 ms  1.290 ms
   2  olp-67-217-152-5.olp.net (67.217.152.5)  1.604 ms  1.859 ms  2.048 ms
   3  router.olp.net (67.217.151.97)  1.531 ms  1.533 ms  1.529 ms
   4  olp-67-217-152-34.olp.net (67.217.152.34)  1.806 ms  1.799 ms  1.781 ms
   5  ae5-694.edge9.dallas1.level3.net (4.30.66.17)  8.856 ms  8.859 ms
 8.838 ms
   6  ae-4-90.edge4.Dallas3.Level3.net (4.69.145.205)  9.350 ms
 ae-1-60.edge4.Dallas3.Level3.net (4.69.145.13)  13.926 ms
 ae-2-70.edge4.Dallas3.Level3.net (4.69.145.77)  15.028 ms
   7  208.175.175.13 (208.175.175.13)  8.923 ms  8.867 ms
 er1-te-3-2.dallasequinix.savvis.net (208.173.178.141)  8.877 ms
   8  dpr1-ge-2-0-0.dallasequinix.savvis.net (204.70.204.146)  8.505 ms
 8.605 ms  8.773 ms
   9  cr2-tengige0-7-5-0.dallas.savvis.net (204.70.196.29)  27.324 ms  27.280
 ms  27.309 ms
 10  cr2-pos-0-0-0-0.chicago.savvis.net (204.70.192.97)  40.620 ms  40.589
 ms  40.603 ms
 11  hr2-tengigabitethernet-12-1.elkgrovech3.savvis.net (204.70.195.122)
 29.226 ms  29.191 ms  27.777 ms
 12  das3-v3039.ch3.savvis.net (64.37.207.186)  28.183 ms  28.141 ms  28.160
 ms
 13  64.27.160.194 (64.27.160.194)  28.342 ms  28.310 ms  28.329 ms
 14  slashdot.org (216.34.181.45)  28.136 ms  28.224 ms  28.220 ms
 dwhite at quark:~$ ping -A -c 100 -q slashdot.org
 PING slashdot.org (216.34.181.45) 56(84) bytes of data.
 --- slashdot.org ping statistics ---
 100 packets transmitted, 100 received, 0% packet loss, time 19859ms
 rtt min/avg/max/mdev = 27.413/27.975/31.337/0.672 ms, ipg/ewma
 200.603/27.854 ms
 -- 
 Dan White
###############################################################
END
###############################################################

###############################################################
Packet Loss Alternet-IAD -> Google-net
###############################################################
 Pretty serious; sustained over 5 minutes of tests:
 HOST: elphaba.baylink.com         Loss%   Snt   Last   Avg  Best  Wrst StDev
   1.|-- L300.TAMPFL-VFTTP-118.ver  0.0%    20    1.3   3.4   1.3  26.5   5.5
   2.|-- G0-5-3-4.TAMPFL-LCR-21.ve  0.0%    20    2.0   5.7   2.0  38.1   8.5
   3.|-- so-5-0-2-0.LCC1-RES-BB-RT  0.0%    20   19.7   7.3   2.1  42.0   9.4
   4.|-- 0.ge-4-1-0.XL3.MIA4.ALTER  0.0%    20    9.6  11.6   9.3  26.8   4.5
   5.|-- 0.xe-4-0-0.XL3.IAD8.ALTER 10.0%    20   42.0  43.7  41.7  55.8   3.1
   6.|-- TenGigE0-4-0-1.GW7.IAD8.A  0.0%    20   39.8  43.0  39.1  46.8   2.7
     |  `|-- 152.63.37.78
     |   |-- 152.63.32.190
     |   |-- 152.63.37.82
     |   |-- 152.63.37.150
     |   |-- 152.63.32.194
   7.|-- google-gw.customer.alter. 50.0%    20  119.3 115.2 110.2 119.3   3.1
   8.|-- 72.14.238.212             65.0%    20   41.2  42.5  39.6  54.5   5.3
   9.|-- 72.14.238.105             25.0%    20   56.4  58.7  55.2  70.7   4.9
  10.|-- 72.14.233.86              25.0%    20   60.5  62.2  59.3  79.2   5.7
  11.|-- 72.14.237.218             30.0%    20   57.3  77.0  57.2 162.6  34.4
  12.|-- 216.239.47.54             55.0%    20   57.7  57.4  56.9  58.1   0.5
  13.|-- dfw06s03-in-f16.1e100.net 30.0%    20   58.3  58.2  56.4  60.0   0.9
 HOST: elphaba.baylink.com         Loss%   Snt   Last   Avg  Best  Wrst StDev
   1.|-- 96.254.123.1               0.0%    20    1.9   2.1   1.4   3.3   0.6
   2.|-- 130.81.131.210             0.0%    20    2.7   4.5   1.7  28.0   6.5
   3.|-- 130.81.28.212              0.0%    20    2.9   5.2   2.0  34.8   7.9
   4.|-- 152.63.4.9                 0.0%    20   10.7  15.6   9.0  97.7  20.3
   5.|-- 152.63.40.233              5.0%    20   42.8  44.9  41.6  67.1   7.6
   6.|-- 152.63.37.82               0.0%    20   44.9  44.1  39.0  46.9   2.3
     |  `|-- 152.63.37.150
     |   |-- 152.63.32.190
     |   |-- 152.63.37.78
     |   |-- 152.63.32.194
     |   |-- 152.63.37.154
   7.|-- 152.179.50.62             55.0%    20  115.0 112.0 106.7 117.4   4.0
   8.|-- 72.14.238.212             65.0%    20   41.0  47.7  39.7  90.9  19.1
   9.|-- 72.14.238.105             50.0%    20   79.0  59.3  55.6  79.0   7.0
  10.|-- 72.14.233.86              35.0%    20   60.2  60.2  59.4  61.2   0.5
  11.|-- 72.14.237.218             40.0%    20   57.6  57.5  56.7  59.6   0.8
  12.|-- 216.239.47.54             30.0%    20   58.0  57.6  56.6  58.3   0.6
  13.|-- 74.125.227.16             40.0%    20   57.3  57.6  56.5  58.6   0.6
 (I do wish mtr had a "show both names and numbers" switch for -r; anyone
 got commit privs?  :-)
 Cheers,
 -- jra
 -- 
 Jay R. Ashworth                  Baylink                       jra at baylink.com
 Designer                     The Things I Think                       RFC 2100
 Ashworth & Associates     http://baylink.pitas.com         2000 Land Rover DII
 St Petersburg FL USA      http://photo.imageinc.us             +1 727 647 1274
###############################################################
END
###############################################################

###############################################################
Google DNS Trouble
###############################################################
 Anyone else having Google DNS problems? Google DNS appears to be very,
 very spotty from our site. Coming from prefix 69.36.128.0/20. I can
 ping the IPs, but DNS lookups mostly timeout. Haven't been able to
 replicate from other sites/prefixes. Haven't been able to find a service
 status page for it either.
 $ dig @8.8.8.8 google.com
 ; <<>> DiG 9.9.0 <<>> @8.8.8.8 google.com
 ; (1 server found)
 ;; global options: +cmd
 ;; connection timed out; no servers could be reached
 $ tracert 8.8.8.8
 Tracing route to google-public-dns-a.google.com [8.8.8.8]
 over a maximum of 30 hops:
   1    <1 ms    <1 ms    <1 ms  [RFC 1918]
   2    <1 ms    <1 ms    <1 ms  [RFC 1918]
   3    <1 ms    <1 ms    <1 ms  [RFC 1918]
   4    <1 ms    <1 ms    <1 ms  69.36.131.227 
   5    <1 ms    <1 ms    <1 ms  ge-11-0-2.mpr2.pao1.us.above.net [64.125.199.237] 
   6     1 ms     1 ms     1 ms  xe-2-2-0.cr2.sjc2.us.above.net [64.125.31.70] 
   7     1 ms     1 ms     1 ms  xe-0-1-0.mpr4.sjc7.us.above.net [64.125.30.178] 
   8     2 ms     2 ms     1 ms  74.125.49.13 
   9     2 ms     2 ms     2 ms  216.239.49.168 
  10     2 ms     2 ms     2 ms  209.85.250.64 
  11     *        *        *     Request timed out.
  12    42 ms    46 ms    23 ms  72.14.233.202 
  13    24 ms    23 ms    23 ms  64.233.174.129 
  14     *        *        *     Request timed out.
  15    23 ms    24 ms    23 ms  google-public-dns-a.google.com [8.8.8.8] 
 Trace complete.
 -- 
 Crist J. Clark
 Seems fine for me.
 C:\Users\jluthman>dig google.com @8.8.8.8 +short
 72.14.204.100
 72.14.204.101
 72.14.204.138
 72.14.204.113
 72.14.204.102
 C:\Users\jluthman>dig random.org @8.8.8.8 +short
 174.143.173.125
 C:\Users\jluthman>dig inxwireless.com @8.8.8.8 +short
 74.218.88.172
 Josh Luthman
 Office: 937-552-2340
 Direct: 937-552-2343
 1100 Wayne St
 Suite 1337
 Troy, OH 45373
 On Tue, Apr 10, 2012 at 5:17 PM, Crist J. Clark <pumpky at sonic.net> wrote:
 Twitter search shows a bunch of people reporting Google DNS trouble over
 the past hour. 
 https://twitter.com/#!/search/realtime/google%20dns
 -george
 -----Original Message-----
 From: outages-bounces at outages.org [mailto:outages-bounces at outages.org]
 On Behalf Of Crist J. Clark
 Sent: Tuesday, April 10, 2012 4:18 PM
 Anyone else having Google DNS problems? Google DNS appears to be very,
 very spotty from our site. Coming from prefix 69.36.128.0/20. I can ping
 the IPs, but DNS lookups mostly timeout. Haven't been able to replicate
 from other sites/prefixes. Haven't been able to find a service status
 page for it either.
 $ dig @8.8.8.8 google.com
 ; <<>> DiG 9.9.0 <<>> @8.8.8.8 google.com ; (1 server found) ;; global
 options: +cmd ;; connection timed out; no servers could be reached
 $ tracert 8.8.8.8
 Tracing route to google-public-dns-a.google.com [8.8.8.8] over a maximum
 of 30 hops:
   1    <1 ms    <1 ms    <1 ms  [RFC 1918]
   2    <1 ms    <1 ms    <1 ms  [RFC 1918]
   3    <1 ms    <1 ms    <1 ms  [RFC 1918]
   4    <1 ms    <1 ms    <1 ms  69.36.131.227 
   5    <1 ms    <1 ms    <1 ms  ge-11-0-2.mpr2.pao1.us.above.net
 [64.125.199.237] 
   6     1 ms     1 ms     1 ms  xe-2-2-0.cr2.sjc2.us.above.net
 [64.125.31.70] 
   7     1 ms     1 ms     1 ms  xe-0-1-0.mpr4.sjc7.us.above.net
 [64.125.30.178] 
   8     2 ms     2 ms     1 ms  74.125.49.13 
   9     2 ms     2 ms     2 ms  216.239.49.168 
  10     2 ms     2 ms     2 ms  209.85.250.64 
  11     *        *        *     Request timed out.
  12    42 ms    46 ms    23 ms  72.14.233.202 
  13    24 ms    23 ms    23 ms  64.233.174.129 
  14     *        *        *     Request timed out.
  15    23 ms    24 ms    23 ms  google-public-dns-a.google.com [8.8.8.8]
 Trace complete.
 --
 Crist J. Clark
 _______________________________________________
 Outages mailing list
 Outages at outages.org
 https://puck.nether.net/mailman/listinfo/outages
 A little slow on the trace from my side but seems to be responding to me as expected from all 3 locations I tested.  Coming from 72.165.142.0/24 173.248.152.0/25 and 75.151.89.0/24.  Seems to slow down when it hits the Google network from Comcast and Centurylink.
 Blake Pfankuch
 -----Original Message-----
 From: outages-bounces at outages.org [mailto:outages-bounces at outages.org] On Behalf Of Crist J. Clark
 Sent: Tuesday, April 10, 2012 3:18 PM
 Anyone else having Google DNS problems? Google DNS appears to be very, very spotty from our site. Coming from prefix 69.36.128.0/20. I can ping the IPs, but DNS lookups mostly timeout. Haven't been able to replicate from other sites/prefixes. Haven't been able to find a service status page for it either.
 $ dig @8.8.8.8 google.com
 ; <<>> DiG 9.9.0 <<>> @8.8.8.8 google.com ; (1 server found) ;; global options: +cmd ;; connection timed out; no servers could be reached
 $ tracert 8.8.8.8
 Tracing route to google-public-dns-a.google.com [8.8.8.8] over a maximum of 30 hops:
   1    <1 ms    <1 ms    <1 ms  [RFC 1918]
   2    <1 ms    <1 ms    <1 ms  [RFC 1918]
   3    <1 ms    <1 ms    <1 ms  [RFC 1918]
   4    <1 ms    <1 ms    <1 ms  69.36.131.227 
   5    <1 ms    <1 ms    <1 ms  ge-11-0-2.mpr2.pao1.us.above.net [64.125.199.237] 
   6     1 ms     1 ms     1 ms  xe-2-2-0.cr2.sjc2.us.above.net [64.125.31.70] 
   7     1 ms     1 ms     1 ms  xe-0-1-0.mpr4.sjc7.us.above.net [64.125.30.178] 
   8     2 ms     2 ms     1 ms  74.125.49.13 
   9     2 ms     2 ms     2 ms  216.239.49.168 
  10     2 ms     2 ms     2 ms  209.85.250.64 
  11     *        *        *     Request timed out.
  12    42 ms    46 ms    23 ms  72.14.233.202 
  13    24 ms    23 ms    23 ms  64.233.174.129 
  14     *        *        *     Request timed out.
  15    23 ms    24 ms    23 ms  google-public-dns-a.google.com [8.8.8.8] 
 Trace complete.
 --
 Crist J. Clark
 _______________________________________________
 Outages mailing list
 Outages at outages.org
 https://puck.nether.net/mailman/listinfo/outages
 Remember that it's anycasted and others aren't going to get directed to the same servers that you are (unless they're in the same area as you are, network wise). It's probably a localized issue.
 --
 Jeremy L. Gaddis
 On Apr 10, 2012, at 5:17 PM, Crist J. Clark wrote:
###############################################################
END
###############################################################

###############################################################
SF South Bay: chronic latency/packet loss between Abovenet/Comcast at Great Oaks
###############################################################
 There's an issue I've been tracking for a few months now pertaining to a
 network link between Abovenet and Comcast which appears to become
 saturated (or impacted negatively in some way) at nearly the same time
 every night, and lasts for numerous hours, then ceases -- on a
 near-daily basis (no exaggeration).
 Latency and packet loss occur during this time, with latency hitting
 150ms (sometimes higher), with packet loss ranging from 0.5% to 2.0%.
 I've been storing periodic traceroutes/mtrs for over a month showing
 this problem, and been tracking start/end times as well.
 Thankfully I own devices/have connectivity on both ends (src and dst,
 thus can provide mtrs/traceroutes from both directions.  Analysis so
 far, done by myself as well as senior network techs at my co-lo
 provider, confirms this issue is with a link between Abovenet/Comcast,
 likely within the San Jose Great Oaks POP (which I'm familiar with as
 part of my job).
 I opened up a ticket on DSLR/BBR's Comcast Direct forum (which only
 Comcast employees can respond to/view tickets for) over a month ago.
 Someone has been viewing it, but nobody has replied except me.
 I've since made the issue public, where (of course) the general Internet
 community does not quite understand how peering arrangements/contracts
 work (people think that any company who has a contract with Abovenet can
 report issues, but that is simply not the case; you must be a POC for
 the transport to report issues with it), nor do they understand how a
 co-lo provider changing route preferencing can impact the provider
 financially (based on billing metrics, etc.).  My co-lo provider is very
 strict with their routing policies, and it has to do with financial
 reasons that are their own business, not mine.
 The public thread is here, which also includes start/end times,
 traceroutes (both directions), and so on.  I update it every day when
 the issue happens, and ~90% of the time edit my posts when the issue
 ends.
 http://www.dslreports.com/forum/r27085601-SF-South-Bay-Chronic-IP-network-latency-nightly
 Anyway, all the technical details aside:
 Is there anyone on this list who works for Comcast who can contact me
 off-list who is willing to investigate this and drive it to completion?
 An alternative would be for someone to contact me off-list with the name
 or Email address of someone (or division) who handles issues like this
 at Comcast.  I'd love for Abovenet to get involved, but I have no
 contractual obligation to them.  (If there is an Abovenet individual who
 is willing to investigate this "pro bono" per se, that would be
 awesome, but I imagine such is often above one's pay grade).
 -- 
 | Jeremy Chadwick                              jdc at parodius.com |
 | Parodius Networking                     http://www.parodius.com/ |
 | UNIX Systems Administrator                 Mountain View, CA, US |
 | Making life hard for others since 1977.             PGP 4BD6C0CB |
 Hi Jeremy,
 When the issue was raised a week or two ago there seemed to be a route
 announcement issue for 72.20.98.67.  When your colo provider changed
 their policy did they update filters with their upstream?
 Cheers, -ren, who will confirm there is no congestion with Abovenet on
 the port in SJC to Comcast.
 On Tue, Apr 10, 2012 at 9:44 PM, Jeremy Chadwick
 <outages at jdc.parodius.com> wrote:
  <CAG5ReHc=7FjsGpxX5v+kgAerEfoZL3tsMuw4VYYwSvSrryN2Bw@mail.gmail.com>
 Hi Ren,
 The issue with my co-lo pertaining to route announcements has actually
 been "dealt with", meaning "this is just how it is".  I'm wondering if I
 can go into details without violating contractual obligations, hmm.
 Yes, I imagine I can, because it becomes quite obvious if I provide
 traceroutes from both directions, and that's public knowledge.
 It appears that my co-lo (BAIS) doesn't actually adjust route
 announcements on a per-IP basis, but they internally have a hashing
 algorithm in place where on a per-IP basis different addresses utilise
 different network paths.  I still have an open ticket with their senior
 networking engineer about this, who has been somewhat "careful" in what
 he tells me, but so far I've basically gotten confirmation that this is
 indeed how they do their load-balancing for customers to balance out
 network traffic between all of their peering providers (Level 3,
 Abovenet, Cogent, and 2-3 others).
 I can provide those examples (to/from different IPs) if you want to see
 them, but that is a separate matter.  There still seems to be a problem
 between Abovenet/Comcast.  Alternate links/paths through my co-lo (e.g.
 BAIS/Cogent) show no problems on the ingress or egress path -- the
 common path seems to be Abovenet/Comcast when there are problems.
 This is what's presently happening right now:
 Source IP: 67.180.84.87
 Dest IP:   72.20.98.124
 === Tue Apr 10 19:09:00 PDT 2012  (1334110140)
 HOST: icarus.home.lan             Loss%   Snt   Rcv  Last   Avg  Best  Wrst
   1.|-- 192.168.1.1                0.0%    40    40   0.3   0.6   0.2   1.5
   2.|-- 67.180.84.1                0.0%    40    40  24.5  22.7  10.4  54.0
   3.|-- 68.85.191.253              0.0%    40    40  10.2  11.1   8.4  25.5
   4.|-- 68.86.143.98               0.0%    40    40  15.6  16.4  11.1  34.7
   5.|-- 68.86.91.5                 0.0%    40    40  14.3  18.7  12.4  49.7
   6.|-- 68.86.87.182               0.0%    40    40  17.1  19.4  14.4  51.6
   7.|-- 4.71.118.45                0.0%    40    40  14.3  23.7  13.0  77.9
   8.|-- 4.69.152.148               0.0%    40    40  67.5  27.1  13.3 128.0
   9.|-- 4.53.16.18                 5.0%    40    38 151.6 153.3 133.3 184.1
  10.|-- 69.163.65.39               2.5%    40    39 176.3 155.8 135.3 198.1
  11.|-- 72.20.98.124               5.0%    40    38 205.3 152.0 129.8 205.3
 === END
 Source IP: 72.20.98.124
 Dest IP:   67.180.84.87
 === Tue Apr 10 19:09:00 PDT 2012  (1334110140)
 HOST: isis.parodius.com           Loss%   Snt   Rcv  Last   Avg  Best  Wrst
   1.|-- 72.20.98.65                0.0%    41    41   0.4   0.4   0.3   0.6
   2.|-- 69.163.64.44               0.0%    40    40   0.4   0.4   0.3   0.5
   3.|-- 69.163.65.49               0.0%    40    40   0.6  10.6   0.4  76.8
   4.|-- 64.124.65.93               0.0%    40    40  65.6   3.6   0.4  65.6
   5.|-- 64.125.28.54               0.0%    40    40   2.8   4.2   0.7  51.7
   6.|-- 64.125.30.126              0.0%    40    40   0.8   1.4   0.7  16.7
   7.|-- 64.125.30.178              0.0%    40    40   1.1   5.8   1.1  65.5
   8.|-- 75.149.228.133             0.0%    40    40 148.7 136.9 117.4 150.2
   9.|-- 68.86.85.65                5.0%    40    38 139.3 135.5 119.9 148.3
  10.|-- 68.86.90.158               2.5%    40    39 141.6 138.1 120.0 149.8
  11.|-- 68.86.143.93               2.5%    40    39 140.3 136.8 120.5 149.8
  12.|-- 68.85.191.250              0.0%    40    40 150.8 144.8 128.7 159.5
  13.|-- 67.180.84.87               0.0%    40    40 146.7 149.6 132.2 173.9
 === END
 $ host 64.125.30.178
 178.30.125.64.in-addr.arpa domain name pointer xe-0-1-0.mpr4.sjc7.us.above.net.
 $ host 75.149.228.133
 133.228.149.75.in-addr.arpa domain name pointer be-10-403-pe01.11greatoaks.ca.ibone.comcast.net.
 So Ren, if you can investigate this, I would be appreciative of it.
 -- 
 | Jeremy Chadwick                              jdc at parodius.com |
 | Parodius Networking                     http://www.parodius.com/ |
 | UNIX Systems Administrator                 Mountain View, CA, US |
 | Making life hard for others since 1977.             PGP 4BD6C0CB |
 On Tue, Apr 10, 2012 at 09:55:33PM -0400, Ren Provo wrote:
  <CAG5ReHc=7FjsGpxX5v+kgAerEfoZL3tsMuw4VYYwSvSrryN2Bw@mail.gmail.com>
  <20120411021258.GA76435@icarus.home.lan>
 A little PTR would help see more clearly, but I believe I see asymmetry here. Also interesting is the ultimate hop at 0% loss in one of those samples. 
 Feel free to put me in my place, but please do so on -discuss. 
 On Apr 10, 2012, at 19:12, Jeremy Chadwick <outages at jdc.parodius.com> wrote:
 >>> There's an issue I've been tracking for a few months now pertaining to a
 >>> network link between Abovenet and Comcast which appears to become
 >>> saturated (or impacted negatively in some way) at nearly the same time
 >>> every night, and lasts for numerous hours, then ceases -- on a
 >>> near-daily basis (no exaggeration).
 >>> 
 >>> Latency and packet loss occur during this time, with latency hitting
 >>> 150ms (sometimes higher), with packet loss ranging from 0.5% to 2.0%.
 >>> I've been storing periodic traceroutes/mtrs for over a month showing
 >>> this problem, and been tracking start/end times as well.
 >>> 
 >>> Thankfully I own devices/have connectivity on both ends (src and dst,
 >>> thus can provide mtrs/traceroutes from both directions. ?Analysis so
 >>> far, done by myself as well as senior network techs at my co-lo
 >>> provider, confirms this issue is with a link between Abovenet/Comcast,
 >>> likely within the San Jose Great Oaks POP (which I'm familiar with as
 >>> part of my job).
 >>> 
 >>> I opened up a ticket on DSLR/BBR's Comcast Direct forum (which only
 >>> Comcast employees can respond to/view tickets for) over a month ago.
 >>> Someone has been viewing it, but nobody has replied except me.
 >>> 
 >>> I've since made the issue public, where (of course) the general Internet
 >>> community does not quite understand how peering arrangements/contracts
 >>> work (people think that any company who has a contract with Abovenet can
 >>> report issues, but that is simply not the case; you must be a POC for
 >>> the transport to report issues with it), nor do they understand how a
 >>> co-lo provider changing route preferencing can impact the provider
 >>> financially (based on billing metrics, etc.). ?My co-lo provider is very
 >>> strict with their routing policies, and it has to do with financial
 >>> reasons that are their own business, not mine.
 >>> 
 >>> The public thread is here, which also includes start/end times,
 >>> traceroutes (both directions), and so on. ?I update it every day when
 >>> the issue happens, and ~90% of the time edit my posts when the issue
 >>> ends.
 >>> 
 >>> http://www.dslreports.com/forum/r27085601-SF-South-Bay-Chronic-IP-network-latency-nightly
 >>> 
 >>> Anyway, all the technical details aside:
 >>> 
 >>> Is there anyone on this list who works for Comcast who can contact me
 >>> off-list who is willing to investigate this and drive it to completion?
 >>> 
 >>> An alternative would be for someone to contact me off-list with the name
 >>> or Email address of someone (or division) who handles issues like this
 >>> at Comcast. ?I'd love for Abovenet to get involved, but I have no
 >>> contractual obligation to them. ?(If there is an Abovenet individual who
 >>> is willing to investigate this "pro bono" per se, that would be
 >>> awesome, but I imagine such is often above one's pay grade).
 >>> 
 >>> --
 >>> | Jeremy Chadwick ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?jdc at parodius.com |
 >>> | Parodius Networking ? ? ? ? ? ? ? ? ? ? http://www.parodius.com/ |
 >>> | UNIX Systems Administrator ? ? ? ? ? ? ? ? Mountain View, CA, US |
 >>> | Making life hard for others since 1977. ? ? ? ? ? ? PGP 4BD6C0CB |
 >>> 
 >>> _______________________________________________
 >>> Outages mailing list
 >>> Outages at outages.org
 >>> https://puck.nether.net/mailman/listinfo/outages
  <CAG5ReHc=7FjsGpxX5v+kgAerEfoZL3tsMuw4VYYwSvSrryN2Bw@mail.gmail.com>
  <20120411021258.GA76435@icarus.home.lan>
  <0568D8C9-289D-405A-A1DB-C497519EA8B6@gmail.com>
 I choose not to do DNS resolution in mtr because otherwise the terminal
 width required to see FQDNs has to be >76 characters, which often upsets
 mailing list folks.  (This for example is one of the few lists on which
 I top-post)
 I'm not so concerned with the packet loss -- for example in the 2nd mtr
 set I showed, the loss only seems to happen at routers, which is almost
 certainly the result of ICMP prioritisation.
 But the latency is a definite problem and is easily noticeable across
 SSH, Remote Desktop, and any other TCP service (i.e. the latency shown
 is not a result of ICMP prioritisation).  src/dst IPs on both sides are
 actual servers/boxes, not routers.
 But you're absolutely right -- asymmetric routing is in place here,
 which means that everyone has to work together, and simultaneously, to
 really figure out where the problem is.  I can only do so much when I
 have little to no visibility into things (e.g. if I had access to BAIS
 and Abovenet and Level 3 and Comcast routers I could figure out where
 the problem is... ;-) )
 I'm currently engaged in a conversation with Comcast engineers about
 this issue.  (Seems my DSLR post got proper attention)
 So far the statement is that they've looked at the interface for the
 Abovenet/Comcast peering point in question, and although it's being
 used/busy, it's not oversaturated.  They also pointed out that the only
 announcements they see for 72.20.96.0/19 are via Level 3 and Cogent,
 thus the issue is likely to be on my co-lo providers' side (e.g. the
 Level 3 <-> BAIS link).  route-views also confirms the same thing, as
 does my place of work (who has peering with Abovenet natively).
 I have a ticket open with my co-lo provider to investigate this ordeal.
 If this does turn out to be a problem with their Level 3 link being
 saturated chronically, then I owe Comcast/Abovenet an apology (welcome
 to one of the complexities with asymmetric routing!), and I'm going to
 have to make some decisions with regards to co-location and so on,
 because the chronic nature of this problem is unacceptable for myself as
 well as my customers.
 -- 
 | Jeremy Chadwick                              jdc at parodius.com |
 | Parodius Networking                     http://www.parodius.com/ |
 | UNIX Systems Administrator                 Mountain View, CA, US |
 | Making life hard for others since 1977.             PGP 4BD6C0CB |
 On Tue, Apr 10, 2012 at 08:50:00PM -0700, Kevin Blackham wrote:
  <CAG5ReHc=7FjsGpxX5v+kgAerEfoZL3tsMuw4VYYwSvSrryN2Bw@mail.gmail.com>
  <20120411021258.GA76435@icarus.home.lan>
  <0568D8C9-289D-405A-A1DB-C497519EA8B6@gmail.com>
  <20120411043304.GA78572@icarus.home.lan>
 Following up to my own post (in bad habit):
 Can some folks here who have peering with Abovenet (preferably with a
 full routing table) verify that you see an announcement for
 72.20.96.0/19 (AS7151) coming via AS6461 (Abovenet)?
 I've confirmed this is the case at my workplace, but I want extra
 eyes/verification.
 Thanks.
 -- 
 | Jeremy Chadwick                              jdc at parodius.com |
 | Parodius Networking                     http://www.parodius.com/ |
 | UNIX Systems Administrator                 Mountain View, CA, US |
 | Making life hard for others since 1977.             PGP 4BD6C0CB |
 On Tue, Apr 10, 2012 at 09:33:04PM -0700, Jeremy Chadwick wrote:
  <CAG5ReHc=7FjsGpxX5v+kgAerEfoZL3tsMuw4VYYwSvSrryN2Bw@mail.gmail.com>
  <20120411021258.GA76435@icarus.home.lan>
  <0568D8C9-289D-405A-A1DB-C497519EA8B6@gmail.com>
  <20120411043304.GA78572@icarus.home.lan>
  <20120411053531.GA79804@icarus.home.lan>
 I guess there's no need for anyone to do this.  I completely forgot that
 Abovenet has a looking glass.
 They absolutely see a route announcement for 72.20.96.0/19 from AS7151,
 including from mpr4.sjc7.us.above.net (keep reading):
 Per http://lg.above.net/lg.cgi --
 Router: mpr4.sjc7.us.above.net
 Command: show route protocol bgp table inet.0 72.20.96.0/19 terse exact
 inet.0: 404034 destinations, 2133715 routes (403943 active, 108 holddown, 1638 hidden)
 Restart Complete
 + = Active Route, - = Last Active, * = Both
 A Destination        P Prf   Metric 1   Metric 2  Next hop        AS path
 * 72.20.96.0/19      B 170        200          0 >64.125.27.94    7151 I
                                                   64.125.27.85
 Peering point confirmation (egress traceroute run from 72.20.98.124
 destined to 67.180.84.87):
 traceroute to 67.180.84.87 (67.180.84.87), 64 hops max, 52 byte packets
  1  72.20.98.65 (72.20.98.65)  0.354 ms  0.232 ms  0.362 ms
  2  er1sc2.bayarea.net (69.163.64.44)  0.363 ms  0.258 ms  0.243 ms
  3  er2sc2.bayarea.net (69.163.65.49)  0.489 ms  0.438 ms *
  4  xe-7-1-0.er1.sjc2.above.net (64.124.65.93)  0.527 ms  0.476 ms  0.488 ms
  5  xe-4-0-0.cr1.sjc2.us.above.net (64.125.28.54)  1.650 ms  0.711 ms  1.087 ms
  6  xe-0-0-0.cr2.sjc2.us.above.net (64.125.30.126)  0.879 ms  0.876 ms  0.735 ms
  7  xe-0-1-0.mpr4.sjc7.us.above.net (64.125.30.178)  1.156 ms  1.104 ms  1.121 ms
  8  be-10-403-pe01.11greatoaks.ca.ibone.comcast.net (75.149.228.133)  7.601 ms  11.717 ms  11.968 ms
  9  pos-2-1-0-0-cr01.sanjose.ca.ibone.comcast.net (68.86.85.65)  6.686 ms  3.310 ms  3.851 ms
 10  pos-0-14-0-0-ar01.sfsutro.ca.sfba.comcast.net (68.86.90.158)  8.716 ms  7.414 ms  8.343 ms
 11  te-9-8-ur03.santaclara.ca.sfba.comcast.net (68.86.143.93)  5.722 ms  5.975 ms  5.698 ms
 12  68.85.191.250 (68.85.191.250)  10.868 ms  13.714 ms  7.968 ms
 13  c-67-180-84-87.hsd1.ca.comcast.net (67.180.84.87)  16.969 ms  48.121 ms  15.588 ms
 So what Comcast's "backbone team" told me appears to be incorrect (we're
 all human), or there are route filters being applied, or they don't get
 a full routing table from Abovenet -- unknown which.  I'm still talking
 to them about that, but probably won't get an answer until later
 tomorrow.
 I still have a ticket open with my co-lo provider to investigate the
 Level 3 link they have.  That's just as much of a possibility of an
 saturation point as the Abovenet/Comcast link is.
 Abovenet's LG also offers ping capability, so I should be able to use
 that as a way to narrow down/confirm if the problem is there or with the
 Level 3<->BAIS link.  Will find out tomorrow...
 -- 
 | Jeremy Chadwick                              jdc at parodius.com |
 | Parodius Networking                     http://www.parodius.com/ |
 | UNIX Systems Administrator                 Mountain View, CA, US |
 | Making life hard for others since 1977.             PGP 4BD6C0CB |
 On Tue, Apr 10, 2012 at 10:35:31PM -0700, Jeremy Chadwick wrote:
  <CAG5ReHc=7FjsGpxX5v+kgAerEfoZL3tsMuw4VYYwSvSrryN2Bw@mail.gmail.com>
  <20120411021258.GA76435@icarus.home.lan>
  <0568D8C9-289D-405A-A1DB-C497519EA8B6@gmail.com>
  <20120411043304.GA78572@icarus.home.lan>
  <20120411053531.GA79804@icarus.home.lan>
  <20120411071227.GA81341@icarus.home.lan>
 On Wed, Apr 11, 2012 at 3:12 AM, Jeremy Chadwick
 <outages at jdc.parodius.com>wrote:
 Their web looking glass isn't enough for this purpose. You need to see at
 least
 - what communities are on the route
 - what the outbound policy is on the session
 - what the inbound policy is on the other side
 All you have here is 'route in table'.
 I still have a ticket open with my co-lo provider to investigate the
 This is what you should have done in the first place; deal with the people
 you pay for service and push them to do their job.
 CC
 -------------- next part --------------
 An HTML attachment was scrubbed...
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120411/fcc7c62e/attachment.html>
  <CAG5ReHc=7FjsGpxX5v+kgAerEfoZL3tsMuw4VYYwSvSrryN2Bw@mail.gmail.com>
  <20120411021258.GA76435@icarus.home.lan>
  <0568D8C9-289D-405A-A1DB-C497519EA8B6@gmail.com>
  <20120411043304.GA78572@icarus.home.lan>
  <20120411053531.GA79804@icarus.home.lan>
  <20120411071227.GA81341@icarus.home.lan>
 Jeremy,
 That your collocation provider is a) load-balancing by hashing traffic
 between two disparate transit ASes (with very different interprovider
 connectivity, performance characteristics, ...), on a per-IP basis b)
 running its transit ports hot, by its own admission... does not
 inspire confidence in its ability to deliver reliable service.
 At this juncture, it seems like we've sufficiently beaten this horse
 and established that there are no known issues between the backbone
 providers you've called out in this thread, from the information
 provided to date.
 The good news about the Bay Area is that it's a competitive
 marketplace, with no shortage of competent providers selling
 collocation and IP.  Probably off-topic for discussion on this list,
 though I'd be happy to recommend some offline if you're coming up
 short.
 Hope this helps,
 -a
 On Wed, Apr 11, 2012 at 3:12 AM, Jeremy Chadwick
 <outages at jdc.parodius.com> wrote:
  <CAG5ReHc=7FjsGpxX5v+kgAerEfoZL3tsMuw4VYYwSvSrryN2Bw@mail.gmail.com>
  <20120411021258.GA76435@icarus.home.lan>
  <0568D8C9-289D-405A-A1DB-C497519EA8B6@gmail.com>
  <20120411043304.GA78572@icarus.home.lan>
  <20120411053531.GA79804@icarus.home.lan>
  <20120411071227.GA81341@icarus.home.lan>
  <CAPZUUwWND+q8XW2gWw4gHoWx8t_k=FfRb9fni60zD+W_kUEPkw@mail.gmail.com>
 On Wed, Apr 11, 2012 at 01:38:49PM -0400, Adam Rothschild wrote:
 Adam,
 Sorry, but it doesn't help.  The recurring latency I've reported is
 quite real.  As I said, I have months of mtrs/traceroutes from both
 directions showing this problem.  Asymmetric routing does not/can not
 explain the chronic high latency seen at roughly the same times nearly
 every day.  I wish it was ICMP prioritisation (I really do).
 I have done as much work as I can on documenting the recurring nature of
 the problem, where its seen (either between Level 3 and AS7151, or
 between Abovenet and Comcast), when it starts, and when it ends.  As I
 am the customer on *both ends* (src and dst), the fact that I'm getting
 no where is preposterous.  Given that I do not have access to Abovenet,
 Comcast, Level 3, or my co-lo providers' routers, I'm forced to rely on
 the competency of others.
 Nobody has "beaten this horse" -- the horse is still there, blocking the
 road, its corpse rotting and festering, affecting network traffic.  Who
 hauls it into the road every day from roughly 17:00 to 21:00 PDT is
 unknown.  Instead, all that's happened is folks focusing on the
 asymmetric aspect of routing, and with my co-lo provider's choice to
 siphon certain IPs through certain routes.
 As for the Bay Area having "no shortage of competent providers selling
 co-location": let me know when that happens.  All I've seen so far is
 complete and total incompetence on the parts of co-lo providers (not
 only our current but our previous as well), transit and peering
 providers, and many other divisions.  Honest: do not get me started on
 this.  Please do not.
 -- 
 | Jeremy Chadwick                              jdc at parodius.com |
 | Parodius Networking                     http://www.parodius.com/ |
 | UNIX Systems Administrator                 Mountain View, CA, US |
 | Making life hard for others since 1977.             PGP 4BD6C0CB |
  <CAG5ReHc=7FjsGpxX5v+kgAerEfoZL3tsMuw4VYYwSvSrryN2Bw@mail.gmail.com>
  <20120411021258.GA76435@icarus.home.lan>
  <0568D8C9-289D-405A-A1DB-C497519EA8B6@gmail.com>
  <20120411043304.GA78572@icarus.home.lan>
  <20120411053531.GA79804@icarus.home.lan>
  <20120411071227.GA81341@icarus.home.lan>
  <CAPZUUwWND+q8XW2gWw4gHoWx8t_k=FfRb9fni60zD+W_kUEPkw@mail.gmail.com>
  <20120411181223.GA94786@icarus.home.lan>
 You are correct.  Comcast has lots of intentional capacity issues.  This is not news.  
 http://lmgtfy.com/?q=comcast+backdoor+santa
 --
 Steve Rubin                                                                  ser at layer42.net
 Layer42 Networks                                                       http://www.layer42.net/
 Expect More from A Web Solutions Provider         (408)450-5742 
###############################################################
END
###############################################################

###############################################################
sun.com name servers unreachable?
###############################################################
 I've checked from multiple networks and recursive name servers. It seems all of the authoritative name servers for sun.com are not responding to requests. Is everyone else seeing this as well?
 Traces look the same to all sun.com name servers I get from glue data via a.gtld-servers.net.
 Tracing route to 192.18.43.12 over a maximum of 30 hops
   1    <1 ms     5 ms    <1 ms  vlan40.s1-lnsskinj.dellservices.net [155.16.137.129]
   2    <1 ms    <1 ms    <1 ms  vlan31.colo3.tdc.nwt.dellservices.net [69.39.183.177]
   3    <1 ms    <1 ms    <1 ms  ge1-6.core2.tdc.nwt.dellservices.net [66.97.0.13]
   4     4 ms     4 ms     4 ms  ge1-9.core1.ccs.phl.dellservices.net [216.182.7.58]
   5     4 ms     4 ms     4 ms  12.116.229.17
   6     9 ms     8 ms     8 ms  cr84.phlpa.ip.att.net [12.123.224.222]
   7    17 ms    11 ms    11 ms  cr2.phlpa.ip.att.net [12.122.107.117]
   8    12 ms    10 ms    12 ms  cr1.wswdc.ip.att.net [12.122.4.54]
   9     7 ms     7 ms     7 ms  gar13.n54ny.ip.att.net [12.122.81.245]
 10     9 ms     9 ms     9 ms  192.205.35.106
 11    10 ms    10 ms    10 ms  te0-1-0-5.mpd22.dca01.atlas.cogentco.com [154.54.26.121]
 12    26 ms    26 ms    27 ms  te0-2-0-4.mpd22.ord01.atlas.cogentco.com [154.54.40.242]
 13    45 ms    45 ms    45 ms  te0-1-0-2.mpd22.mci01.atlas.cogentco.com [154.54.3.202]
 14    57 ms    57 ms    57 ms  te4-4.mpd02.den01.atlas.cogentco.com [154.54.45.49]
 15    57 ms    57 ms    57 ms  te3-2.ccr01.den03.atlas.cogentco.com [154.54.83.34]
 16    80 ms    80 ms    80 ms  38.122.114.46
 17    80 ms    80 ms    80 ms  mpr2.den.ve3-bbnet2.pnap.net [216.52.40.72]
 18     *        *        *     Request timed out.
 -Vinny
 -------------- next part --------------
 An HTML attachment was scrubbed...
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120411/2e60a473/attachment.html>
 Perhaps by design?
 http://www.oracle.com/us/sun/index.htm
 On 11/04/2012, at 2:07 PM, <Vinny_Abello at Dell.com>
  wrote:
 I?ve checked from multiple networks and recursive name servers. It seems all of the authoritative name servers for sun.com<http://sun.com> are not responding to requests. Is everyone else seeing this as well?
 Traces look the same to all sun.com<http://sun.com> name servers I get from glue data via a.gtld-servers.net<http://a.gtld-servers.net>.
 Tracing route to 192.18.43.12 over a maximum of 30 hops
   1    <1 ms     5 ms    <1 ms  vlan40.s1-lnsskinj.dellservices.net<http://vlan40.s1-lnsskinj.dellservices.net> [155.16.137.129]
   2    <1 ms    <1 ms    <1 ms  vlan31.colo3.tdc.nwt.dellservices.net [69.39.183.177]
   3    <1 ms    <1 ms    <1 ms  ge1-6.core2.tdc.nwt.dellservices.net [66.97.0.13]
   4     4 ms     4 ms     4 ms  ge1-9.core1.ccs.phl.dellservices.net [216.182.7.58]
   5     4 ms     4 ms     4 ms  12.116.229.17
   6     9 ms     8 ms     8 ms  cr84.phlpa.ip.att.net [12.123.224.222]
   7    17 ms    11 ms    11 ms  cr2.phlpa.ip.att.net [12.122.107.117]
   8    12 ms    10 ms    12 ms  cr1.wswdc.ip.att.net [12.122.4.54]
   9     7 ms     7 ms     7 ms  gar13.n54ny.ip.att.net [12.122.81.245]
 10     9 ms     9 ms     9 ms  192.205.35.106
 11    10 ms    10 ms    10 ms  te0-1-0-5.mpd22.dca01.atlas.cogentco.com [154.54.26.121]
 12    26 ms    26 ms    27 ms  te0-2-0-4.mpd22.ord01.atlas.cogentco.com [154.54.40.242]
 13    45 ms    45 ms    45 ms  te0-1-0-2.mpd22.mci01.atlas.cogentco.com [154.54.3.202]
 14    57 ms    57 ms    57 ms  te4-4.mpd02.den01.atlas.cogentco.com [154.54.45.49]
 15    57 ms    57 ms    57 ms  te3-2.ccr01.den03.atlas.cogentco.com [154.54.83.34]
 16    80 ms    80 ms    80 ms  38.122.114.46
 17    80 ms    80 ms    80 ms  mpr2.den.ve3-bbnet2.pnap.net [216.52.40.72]
 18     *        *        *     Request timed out.
 -Vinny
 _______________________________________________
 Outages mailing list
 Outages at outages.org<mailto:Outages at outages.org>
 https://puck.nether.net/mailman/listinfo/outages
 -------------- next part --------------
 An HTML attachment was scrubbed...
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120411/6f21a344/attachment.html>
 Old news finally comes to fruition?
 http://www.theregister.co.uk/2011/03/16/oracle_closes_sun_domain/
 On 4/11/12 12:37 AM, Vinny_Abello at Dell.com wrote:
 -------------- next part --------------
 An HTML attachment was scrubbed...
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120411/4eff0804/attachment-0001.html>
 -------------- next part --------------
 A non-text attachment was scrubbed...
 Name: smime.p7s
 Type: application/pkcs7-signature
 Size: 4899 bytes
 Desc: S/MIME Cryptographic Signature
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120411/4eff0804/attachment-0001.p7s>
  <4F85105C.6050700@vantage.com>
 Quite possibly! However, someone running java.com messed up as nobody can download java anymore as a result. J
 -Vinny
 From: Jon Radel [mailto:jradel at vantage.com]
 Sent: Wednesday, April 11, 2012 1:02 AM
 Cc: outages at outages.org
 Old news finally comes to fruition?
 http://www.theregister.co.uk/2011/03/16/oracle_closes_sun_domain/
 On 4/11/12 12:37 AM, Vinny_Abello at Dell.com<mailto:Vinny_Abello at Dell.com> wrote:
 I've checked from multiple networks and recursive name servers. It seems all of the authoritative name servers for sun.com are not responding to requests. Is everyone else seeing this as well?
 Traces look the same to all sun.com name servers I get from glue data via a.gtld-servers.net.
 Tracing route to 192.18.43.12 over a maximum of 30 hops
   1    <1 ms     5 ms    <1 ms  vlan40.s1-lnsskinj.dellservices.net [155.16.137.129]
   2    <1 ms    <1 ms    <1 ms  vlan31.colo3.tdc.nwt.dellservices.net [69.39.183.177]
   3    <1 ms    <1 ms    <1 ms  ge1-6.core2.tdc.nwt.dellservices.net [66.97.0.13]
   4     4 ms     4 ms     4 ms  ge1-9.core1.ccs.phl.dellservices.net [216.182.7.58]
   5     4 ms     4 ms     4 ms  12.116.229.17
   6     9 ms     8 ms     8 ms  cr84.phlpa.ip.att.net [12.123.224.222]
   7    17 ms    11 ms    11 ms  cr2.phlpa.ip.att.net [12.122.107.117]
   8    12 ms    10 ms    12 ms  cr1.wswdc.ip.att.net [12.122.4.54]
   9     7 ms     7 ms     7 ms  gar13.n54ny.ip.att.net [12.122.81.245]
 10     9 ms     9 ms     9 ms  192.205.35.106
 11    10 ms    10 ms    10 ms  te0-1-0-5.mpd22.dca01.atlas.cogentco.com [154.54.26.121]
 12    26 ms    26 ms    27 ms  te0-2-0-4.mpd22.ord01.atlas.cogentco.com [154.54.40.242]
 13    45 ms    45 ms    45 ms  te0-1-0-2.mpd22.mci01.atlas.cogentco.com [154.54.3.202]
 14    57 ms    57 ms    57 ms  te4-4.mpd02.den01.atlas.cogentco.com [154.54.45.49]
 15    57 ms    57 ms    57 ms  te3-2.ccr01.den03.atlas.cogentco.com [154.54.83.34]
 16    80 ms    80 ms    80 ms  38.122.114.46
 17    80 ms    80 ms    80 ms  mpr2.den.ve3-bbnet2.pnap.net [216.52.40.72]
 18     *        *        *     Request timed out.
 -Vinny
 _______________________________________________
 Outages mailing list
 Outages at outages.org<mailto:Outages at outages.org>
 https://puck.nether.net/mailman/listinfo/outages
 -------------- next part --------------
 An HTML attachment was scrubbed...
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120411/b400f7e7/attachment.html>
  <4F85105C.6050700@vantage.com>
  <FD9B2CB2B33E394FAE3B74669547605712B76643@DFWX10HMPTC01.AMER.DELL.COM>
 Oracle did disable the sun.com domain (although not at the DNS server
 level) close to a year ago.  It lasted a few hours before they discovered a
 large number of other websites stopped working as a result, including parts
 of java.com.
 I doubt they would have made the same mistake twice, which leads me to
 guess this is probably an outage rather than a deliberate shutdown.
   Scott.
 On Tue, Apr 10, 2012 at 10:11 PM, <Vinny_Abello at dell.com> wrote:
 -------------- next part --------------
 An HTML attachment was scrubbed...
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120410/556a3568/attachment-0001.html>
  <4F85105C.6050700@vantage.com>
  <FD9B2CB2B33E394FAE3B74669547605712B76643@DFWX10HMPTC01.AMER.DELL.COM>
  <CACnPsNV3Y6fQLuegxDhQizMGxdk-pd7ZuJJouwBsrtQDwTV=BA@mail.gmail.com>
 I am seeing this also.    No DNS response from  any of the 4 DNS server IPs.
 $ traceroute -n 192.18.43.12
 traceroute to 192.18.43.12 (192.18.43.12), 30 hops max, 40 byte packets
  1  72.20.181.1  0.454 ms  0.452 ms  0.518 ms
  2  72.20.191.132  10.312 ms  10.435 ms  10.452 ms
  3  72.5.252.77  10.242 ms  10.224 ms  10.324 ms
  4  216.52.191.40  11.328 ms 216.52.191.105  11.599 ms  11.676 ms
  5  38.104.204.73  26.291 ms  26.195 ms  26.156 ms
  6  154.54.6.97  22.448 ms  22.424 ms  22.867 ms
  7  154.54.46.217  22.423 ms 154.54.2.230  23.002 ms 154.54.46.217  22.388 ms
  8  154.54.82.213  34.333 ms  34.261 ms  34.374 ms
  9  154.54.83.34  34.809 ms  34.651 ms  34.513 ms
 10  38.122.114.46  29.532 ms  29.206 ms  29.192 ms
 11  216.52.40.7  29.120 ms  29.081 ms  29.136 ms
 12  * * *
 13  * * *
 14  * * *
 $ traceroute -n 192.18.4.206
 traceroute to 192.18.4.206 (192.18.4.206), 30 hops max, 40 byte packets
  1  72.20.181.1  0.485 ms  0.470 ms  0.520 ms
  2  72.20.191.132  10.405 ms  10.461 ms  10.466 ms
  3  72.5.252.77  10.297 ms  10.340 ms  10.314 ms
  4  216.52.191.40  29.791 ms 216.52.191.105  30.098 ms  29.843 ms
  5  38.104.204.73  123.796 ms  123.795 ms  123.797 ms
  6  154.54.82.29  22.665 ms  22.773 ms  22.746 ms
  7  154.54.46.209  23.023 ms 154.54.80.106  22.189 ms  22.257 ms
  8  154.54.82.213  93.193 ms 154.54.82.209  93.199 ms  93.172 ms
  9  154.54.45.186  37.295 ms  34.561 ms 154.54.83.34  34.584 ms
 10  38.122.114.46  29.169 ms  28.993 ms  29.105 ms
 11  216.52.40.7  29.073 ms  28.926 ms  28.843 ms
 12  * * *
 13  * * *
 14  * * *
 $ dig +norec -t A sun.com @a.gtld-servers.net
 ; <<>> DiG 9.6.0-APPLE-P2 <<>> +norec -t A sun.com @a.gtld-servers.net
 ;; global options: +cmd
 ;; Got answer:
 ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 48012
 ;; flags: qr; QUERY: 1, ANSWER: 0, AUTHORITY: 4, ADDITIONAL: 4
 ;; QUESTION SECTION:
 ;sun.com.			IN	A
 ;; AUTHORITY SECTION:
 sun.com.		172800	IN	NS	ns8.sun.com.
 sun.com.		172800	IN	NS	ns2.sun.com.
 sun.com.		172800	IN	NS	ns1.sun.com.
 sun.com.		172800	IN	NS	ns3.sun.com.
 ;; ADDITIONAL SECTION:
 ns8.sun.com.		172800	IN	A	192.18.43.12
 ns2.sun.com.		172800	IN	A	192.18.99.5
 ns1.sun.com.		172800	IN	A	192.18.128.11
 ns3.sun.com.		172800	IN	A	192.18.4.206
 $ dig +norec -t SOA sun.com @192.18.43.12
 ; <<>> DiG 9.6.0-APPLE-P2 <<>> +norec -t SOA sun.com @192.18.43.12
 ;; global options: +cmd
 ;; connection timed out; no servers could be reached
 $ dig +norec -t SOA sun.com @192.18.99.5
 ; <<>> DiG 9.6.0-APPLE-P2 <<>> +norec -t SOA sun.com @192.18.99.5
 ;; global options: +cmd
 ;; connection timed out; no servers could be reached
 $ dig +norec -t SOA sun.com @192.18.128.11
 ; <<>> DiG 9.6.0-APPLE-P2 <<>> +norec -t SOA sun.com @192.18.128.11
 ;; global options: +cmd
 ;; connection timed out; no servers could be reached
 $ dig +norec -t SOA sun.com @192.18.4.206
 ; <<>> DiG 9.6.0-APPLE-P2 <<>> +norec -t SOA sun.com @192.18.4.206
 ;; global options: +cmd
 ;; connection timed out; no servers could be reached
  <4F85105C.6050700@vantage.com>
  <FD9B2CB2B33E394FAE3B74669547605712B76643@DFWX10HMPTC01.AMER.DELL.COM>
  <CACnPsNV3Y6fQLuegxDhQizMGxdk-pd7ZuJJouwBsrtQDwTV=BA@mail.gmail.com>
  <CAAAwwbUtgrF9cLo_P=rBh+3N_kSh5_MeYrmndqXVvuQHXhtuUw@mail.gmail.com>
 Same here.  One of their four DNS servers, ns1.sun.com (192.18.128.11),
 doesn't have Cogent in the path (see
 http://bgptables.merit.edu/assum.php?z=&as=11479#prefixTables), but it makes
 no difference.  None of the traceroutes I perform hit a 192.18.x.x address.
 Frank
 -----Original Message-----
 From: outages-bounces at outages.org [mailto:outages-bounces at outages.org] On
 Behalf Of Jimmy Hess
 Sent: Wednesday, April 11, 2012 1:01 AM
 I am seeing this also.    No DNS response from  any of the 4 DNS server IPs.
 $ traceroute -n 192.18.43.12
 traceroute to 192.18.43.12 (192.18.43.12), 30 hops max, 40 byte packets
  1  72.20.181.1  0.454 ms  0.452 ms  0.518 ms
  2  72.20.191.132  10.312 ms  10.435 ms  10.452 ms
  3  72.5.252.77  10.242 ms  10.224 ms  10.324 ms
  4  216.52.191.40  11.328 ms 216.52.191.105  11.599 ms  11.676 ms
  5  38.104.204.73  26.291 ms  26.195 ms  26.156 ms
  6  154.54.6.97  22.448 ms  22.424 ms  22.867 ms
  7  154.54.46.217  22.423 ms 154.54.2.230  23.002 ms 154.54.46.217  22.388
 ms
  8  154.54.82.213  34.333 ms  34.261 ms  34.374 ms
  9  154.54.83.34  34.809 ms  34.651 ms  34.513 ms
 10  38.122.114.46  29.532 ms  29.206 ms  29.192 ms
 11  216.52.40.7  29.120 ms  29.081 ms  29.136 ms
 12  * * *
 13  * * *
 14  * * *
 $ traceroute -n 192.18.4.206
 traceroute to 192.18.4.206 (192.18.4.206), 30 hops max, 40 byte packets
  1  72.20.181.1  0.485 ms  0.470 ms  0.520 ms
  2  72.20.191.132  10.405 ms  10.461 ms  10.466 ms
  3  72.5.252.77  10.297 ms  10.340 ms  10.314 ms
  4  216.52.191.40  29.791 ms 216.52.191.105  30.098 ms  29.843 ms
  5  38.104.204.73  123.796 ms  123.795 ms  123.797 ms
  6  154.54.82.29  22.665 ms  22.773 ms  22.746 ms
  7  154.54.46.209  23.023 ms 154.54.80.106  22.189 ms  22.257 ms
  8  154.54.82.213  93.193 ms 154.54.82.209  93.199 ms  93.172 ms
  9  154.54.45.186  37.295 ms  34.561 ms 154.54.83.34  34.584 ms
 10  38.122.114.46  29.169 ms  28.993 ms  29.105 ms
 11  216.52.40.7  29.073 ms  28.926 ms  28.843 ms
 12  * * *
 13  * * *
 14  * * *
 $ dig +norec -t A sun.com @a.gtld-servers.net
 ; <<>> DiG 9.6.0-APPLE-P2 <<>> +norec -t A sun.com @a.gtld-servers.net
 ;; global options: +cmd
 ;; Got answer:
 ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 48012
 ;; flags: qr; QUERY: 1, ANSWER: 0, AUTHORITY: 4, ADDITIONAL: 4
 ;; QUESTION SECTION:
 ;sun.com.			IN	A
 ;; AUTHORITY SECTION:
 sun.com.		172800	IN	NS	ns8.sun.com.
 sun.com.		172800	IN	NS	ns2.sun.com.
 sun.com.		172800	IN	NS	ns1.sun.com.
 sun.com.		172800	IN	NS	ns3.sun.com.
 ;; ADDITIONAL SECTION:
 ns8.sun.com.		172800	IN	A	192.18.43.12
 ns2.sun.com.		172800	IN	A	192.18.99.5
 ns1.sun.com.		172800	IN	A	192.18.128.11
 ns3.sun.com.		172800	IN	A	192.18.4.206
 $ dig +norec -t SOA sun.com @192.18.43.12
 ; <<>> DiG 9.6.0-APPLE-P2 <<>> +norec -t SOA sun.com @192.18.43.12
 ;; global options: +cmd
 ;; connection timed out; no servers could be reached
 $ dig +norec -t SOA sun.com @192.18.99.5
 ; <<>> DiG 9.6.0-APPLE-P2 <<>> +norec -t SOA sun.com @192.18.99.5
 ;; global options: +cmd
 ;; connection timed out; no servers could be reached
 $ dig +norec -t SOA sun.com @192.18.128.11
 ; <<>> DiG 9.6.0-APPLE-P2 <<>> +norec -t SOA sun.com @192.18.128.11
 ;; global options: +cmd
 ;; connection timed out; no servers could be reached
 $ dig +norec -t SOA sun.com @192.18.4.206
 ; <<>> DiG 9.6.0-APPLE-P2 <<>> +norec -t SOA sun.com @192.18.4.206
 ;; global options: +cmd
 ;; connection timed out; no servers could be reached
 _______________________________________________
 Outages mailing list
 Outages at outages.org
 https://puck.nether.net/mailman/listinfo/outages
  <4F85105C.6050700@vantage.com>
  <FD9B2CB2B33E394FAE3B74669547605712B76643@DFWX10HMPTC01.AMER.DELL.COM>
  <CACnPsNV3Y6fQLuegxDhQizMGxdk-pd7ZuJJouwBsrtQDwTV=BA@mail.gmail.com>
  <CAAAwwbUtgrF9cLo_P=rBh+3N_kSh5_MeYrmndqXVvuQHXhtuUw@mail.gmail.com>
  <037201cd17a9$9cec82f0$d6c588d0$@iname.com>
 Looks like 192.18.43.12 (ns8.sun.com) is accessible for me now, but the
 other three aren't.
 Frank
 -----Original Message-----
 From: outages-bounces at outages.org [mailto:outages-bounces at outages.org] On
 Behalf Of Frank Bulk
 Sent: Wednesday, April 11, 2012 1:09 AM
 Same here.  One of their four DNS servers, ns1.sun.com (192.18.128.11),
 doesn't have Cogent in the path (see
 http://bgptables.merit.edu/assum.php?z=&as=11479#prefixTables), but it makes
 no difference.  None of the traceroutes I perform hit a 192.18.x.x address.
 Frank
 -----Original Message-----
 From: outages-bounces at outages.org [mailto:outages-bounces at outages.org] On
 Behalf Of Jimmy Hess
 Sent: Wednesday, April 11, 2012 1:01 AM
 I am seeing this also.    No DNS response from  any of the 4 DNS server IPs.
 $ traceroute -n 192.18.43.12
 traceroute to 192.18.43.12 (192.18.43.12), 30 hops max, 40 byte packets
  1  72.20.181.1  0.454 ms  0.452 ms  0.518 ms
  2  72.20.191.132  10.312 ms  10.435 ms  10.452 ms
  3  72.5.252.77  10.242 ms  10.224 ms  10.324 ms
  4  216.52.191.40  11.328 ms 216.52.191.105  11.599 ms  11.676 ms
  5  38.104.204.73  26.291 ms  26.195 ms  26.156 ms
  6  154.54.6.97  22.448 ms  22.424 ms  22.867 ms
  7  154.54.46.217  22.423 ms 154.54.2.230  23.002 ms 154.54.46.217  22.388
 ms
  8  154.54.82.213  34.333 ms  34.261 ms  34.374 ms
  9  154.54.83.34  34.809 ms  34.651 ms  34.513 ms
 10  38.122.114.46  29.532 ms  29.206 ms  29.192 ms
 11  216.52.40.7  29.120 ms  29.081 ms  29.136 ms
 12  * * *
 13  * * *
 14  * * *
 $ traceroute -n 192.18.4.206
 traceroute to 192.18.4.206 (192.18.4.206), 30 hops max, 40 byte packets
  1  72.20.181.1  0.485 ms  0.470 ms  0.520 ms
  2  72.20.191.132  10.405 ms  10.461 ms  10.466 ms
  3  72.5.252.77  10.297 ms  10.340 ms  10.314 ms
  4  216.52.191.40  29.791 ms 216.52.191.105  30.098 ms  29.843 ms
  5  38.104.204.73  123.796 ms  123.795 ms  123.797 ms
  6  154.54.82.29  22.665 ms  22.773 ms  22.746 ms
  7  154.54.46.209  23.023 ms 154.54.80.106  22.189 ms  22.257 ms
  8  154.54.82.213  93.193 ms 154.54.82.209  93.199 ms  93.172 ms
  9  154.54.45.186  37.295 ms  34.561 ms 154.54.83.34  34.584 ms
 10  38.122.114.46  29.169 ms  28.993 ms  29.105 ms
 11  216.52.40.7  29.073 ms  28.926 ms  28.843 ms
 12  * * *
 13  * * *
 14  * * *
 $ dig +norec -t A sun.com @a.gtld-servers.net
 ; <<>> DiG 9.6.0-APPLE-P2 <<>> +norec -t A sun.com @a.gtld-servers.net
 ;; global options: +cmd
 ;; Got answer:
 ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 48012
 ;; flags: qr; QUERY: 1, ANSWER: 0, AUTHORITY: 4, ADDITIONAL: 4
 ;; QUESTION SECTION:
 ;sun.com.			IN	A
 ;; AUTHORITY SECTION:
 sun.com.		172800	IN	NS	ns8.sun.com.
 sun.com.		172800	IN	NS	ns2.sun.com.
 sun.com.		172800	IN	NS	ns1.sun.com.
 sun.com.		172800	IN	NS	ns3.sun.com.
 ;; ADDITIONAL SECTION:
 ns8.sun.com.		172800	IN	A	192.18.43.12
 ns2.sun.com.		172800	IN	A	192.18.99.5
 ns1.sun.com.		172800	IN	A	192.18.128.11
 ns3.sun.com.		172800	IN	A	192.18.4.206
 $ dig +norec -t SOA sun.com @192.18.43.12
 ; <<>> DiG 9.6.0-APPLE-P2 <<>> +norec -t SOA sun.com @192.18.43.12
 ;; global options: +cmd
 ;; connection timed out; no servers could be reached
 $ dig +norec -t SOA sun.com @192.18.99.5
 ; <<>> DiG 9.6.0-APPLE-P2 <<>> +norec -t SOA sun.com @192.18.99.5
 ;; global options: +cmd
 ;; connection timed out; no servers could be reached
 $ dig +norec -t SOA sun.com @192.18.128.11
 ; <<>> DiG 9.6.0-APPLE-P2 <<>> +norec -t SOA sun.com @192.18.128.11
 ;; global options: +cmd
 ;; connection timed out; no servers could be reached
 $ dig +norec -t SOA sun.com @192.18.4.206
 ; <<>> DiG 9.6.0-APPLE-P2 <<>> +norec -t SOA sun.com @192.18.4.206
 ;; global options: +cmd
 ;; connection timed out; no servers could be reached
 _______________________________________________
 Outages mailing list
 Outages at outages.org
 https://puck.nether.net/mailman/listinfo/outages
 _______________________________________________
 Outages mailing list
 Outages at outages.org
 https://puck.nether.net/mailman/listinfo/outages
###############################################################
END
###############################################################

###############################################################
Intermittent packet loss Level3 Dallas
###############################################################
 I've been noticing intermittent packet loss on our Level3 IP service
 in Dallas.  It usually happens for about 5 minutes with ~10% packet
 loss on our link to Level3 in Dallas.
 I was told they did some maintenance last night (bgp session did drop)
 but it has still been occurring.
 Has anyone else been seeing this?
 -mark
###############################################################
END
###############################################################

###############################################################
Cogent outage Chicago area
###############################################################
 Outage started at 11:58 pm CST last night. Just got confirmation from
 Cogent. No ETA on when this would be fixed.
 Andres
 The issue is a local problem in Cogent ring around the North branch Chicago
 river that now is also involving ComEd. According to Cogent, customers
 affected are only those near the north branch Chicago river near the loop.
 Can anyone in Cogent confirm this?
 Andres
 On Mon, Apr 16, 2012 at 8:00 AM, Andres Herrera <aher85 at gmail.com> wrote:
 -------------- next part --------------
 An HTML attachment was scrubbed...
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120416/b4d35115/attachment.html>
  <CAArM3xz-b4xKEsn=w3E+M11oV6o5UPhJqkTpBDR6=LvwikgP6g@mail.gmail.com>
 It's not just limited to Cogent either.  Reliance Globalcom is
 experiencing a seemingly related issue.
 John
 On Mon, Apr 16, 2012 at 09:37:04AM -0500, Andres Herrera wrote:
  <CAArM3xz-b4xKEsn=w3E+M11oV6o5UPhJqkTpBDR6=LvwikgP6g@mail.gmail.com>
  <CABk_raGK2BVPv0kxmb2diMN9VBWuyMOT7vgyZrCj9YzNfvR60A@mail.gmail.com>
 Adam, I can see the Civic Opera building from my office, its right in front
 of me, but i'm on the other side of the river.
 My ticket with Cogent was updated and a faulty fiber link underneath the
 river is what is causing the outage.
 On Mon, Apr 16, 2012 at 10:59 AM, Adam Leff <adam at leff.co> wrote:
 >>> Outage started at 11:58 pm CST last night. Just got confirmation from
 >>> Cogent. No ETA on when this would be fixed.
 >>>
 >>> Andres
 >>>
 -------------- next part --------------
 An HTML attachment was scrubbed...
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120416/c57bbc47/attachment.html>
  <CAArM3xz-b4xKEsn=w3E+M11oV6o5UPhJqkTpBDR6=LvwikgP6g@mail.gmail.com>
  <CABk_raGK2BVPv0kxmb2diMN9VBWuyMOT7vgyZrCj9YzNfvR60A@mail.gmail.com>
  <CAArM3xzpPh-9wfQWCDVbtaztziyKF5YCPrccKwsRHM6WLTnJ5A@mail.gmail.com>
 We are eat of the north/south portion of the river in the south loop:
 Our Cogent transit/peering is currently unaffected
 On 04/16/2012 11:10 AM, Andres Herrera wrote:
 -- 
 sjk at cupacoffee.net
 fingerprint: 1024D/89420B8E 2001-09-16
 No one can understand the truth until
 he drinks of coffee's frothy goodness.
 ~Sheik Abd-al-Kadir
  <CAArM3xz-b4xKEsn=w3E+M11oV6o5UPhJqkTpBDR6=LvwikgP6g@mail.gmail.com>
  <CABk_raGK2BVPv0kxmb2diMN9VBWuyMOT7vgyZrCj9YzNfvR60A@mail.gmail.com>
  <CAArM3xzpPh-9wfQWCDVbtaztziyKF5YCPrccKwsRHM6WLTnJ5A@mail.gmail.com>
 Outage appears to be related to a manhole fire:
 """
 http://status.cogentco.com
 Welcome to the Cogent Communications status page. At this time 4:15 PM
 04/16/2012 some of our Chicago customers may be experiencing loss of
 service due to a manhole fire which has damaged power, phone and fiber
 cable. Our fiber vendor has given us a best case ETR of Tuesday evening.
 The Master ticket for this priority issue is HD5080477.
 """
 John
 On Mon, Apr 16, 2012 at 11:10:11AM -0500, Andres Herrera wrote:
###############################################################
END
###############################################################

###############################################################
cBeyond Phone service in Northern Colorado (might be larger)
###############################################################
 Anyone else having cBeyond issues?  Our PRI is down, as our several customers I spot checked.  Their support line says "We are experiencing an extremely high call volume" and directs you to their website.  Their website currently says "This portion of the Cbeyond portal is currently down for system maintenance or is experiencing technical difficulties. Please try again shortly to complete your task. If you need immediate assistance, please call Cbeyond Customer Care."
 Looks like a big issue, anyone have any more details?
 Blake Pfankuch
 Connecting Point of Greeley
 Project Engineer
 (970) 356-7224 main
 bpfankuch at cpgreeley.com<mailto:bpfankuch at cpgreeley.com>
 [Description: Description: Description: cid:image005.jpg at 01CC46E7.6CB7B430]<http://www.cpgreeley.com/>
 Cisco, Microsoft, Adtran and VMware Certification Information available upon request.
 -------------- next part --------------
 An HTML attachment was scrubbed...
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120419/cef9c269/attachment.html>
 -------------- next part --------------
 A non-text attachment was scrubbed...
 Name: image001.jpg
 Type: image/jpeg
 Size: 2563 bytes
 Desc: image001.jpg
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120419/cef9c269/attachment.jpg>
 Looks like this is affecting all of their markets.  Master Ticket number is 1-199-5233675.
 Blake Pfankuch
 From: outages-bounces at outages.org [mailto:outages-bounces at outages.org] On Behalf Of Blake Pfankuch
 Sent: Thursday, April 19, 2012 10:24 AM
 Anyone else having cBeyond issues?  Our PRI is down, as our several customers I spot checked.  Their support line says "We are experiencing an extremely high call volume" and directs you to their website.  Their website currently says "This portion of the Cbeyond portal is currently down for system maintenance or is experiencing technical difficulties. Please try again shortly to complete your task. If you need immediate assistance, please call Cbeyond Customer Care."
 Looks like a big issue, anyone have any more details?
 Blake Pfankuch
 Connecting Point of Greeley
 Project Engineer
 (970) 356-7224 main
 bpfankuch at cpgreeley.com<mailto:bpfankuch at cpgreeley.com>
 [Description: Description: Description: cid:image005.jpg at 01CC46E7.6CB7B430]<http://www.cpgreeley.com/>
 Cisco, Microsoft, Adtran and VMware Certification Information available upon request.
 -------------- next part --------------
 An HTML attachment was scrubbed...
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120419/d5d406a7/attachment.html>
 -------------- next part --------------
 A non-text attachment was scrubbed...
 Name: image001.jpg
 Type: image/jpeg
 Size: 2563 bytes
 Desc: image001.jpg
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120419/d5d406a7/attachment.jpg>
###############################################################
END
###############################################################

###############################################################
Rightnow reporting several lines doen in bay area
###############################################################
 Anyone got any further info, they are pointing the finger at their
 providers
 -- 
 Martin
 -- 
 -- 
 Martin Hepworth
 Oxford, UK
 -------------- next part --------------
 An HTML attachment was scrubbed...
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120419/418ca6c7/attachment.html>
 We have received a report from Zuora stating the following:
 --
 Anthony Ledesma | FireHost
 From: outages-bounces at outages.org [mailto:outages-bounces at outages.org] On Behalf Of Martin Hepworth
 Sent: Thursday, April 19, 2012 12:43 PM
 Anyone got any further info, they are pointing the finger at their providers?
 --?
 Martin
 -- 
 -- 
 Martin Hepworth
 Oxford, UK
  <66DB42B5BE615F4EB8E5E9A88CEBA9FA0603D32A73@DFW1MBX07.mex07a.mlsrvr.com>
 There is a large fiber cut in Los Angeles affecting many providers at this time that's been going on since earlier today. I'm also being affected by this between LA and Dallas and seeing latency on other networks in the LA area as a result of congestion due to decreased capacity.
 -Vinny
 -----Original Message-----
 From: outages-bounces at outages.org [mailto:outages-bounces at outages.org] On Behalf Of Anthony Ledesma
 Sent: Thursday, April 19, 2012 3:52 PM
 We have received a report from Zuora stating the following:
 --
 Anthony Ledesma | FireHost
 From: outages-bounces at outages.org [mailto:outages-bounces at outages.org] On Behalf Of Martin Hepworth
 Sent: Thursday, April 19, 2012 12:43 PM
 Anyone got any further info, they are pointing the finger at their providers?
 --?
 Martin
 -- 
 -- 
 Martin Hepworth
 Oxford, UK
 _______________________________________________
 Outages mailing list
 Outages at outages.org
 https://puck.nether.net/mailman/listinfo/outages
  <66DB42B5BE615F4EB8E5E9A88CEBA9FA0603D32A73@DFW1MBX07.mex07a.mlsrvr.com>
  <FD9B2CB2B33E394FAE3B74669547605712B7EA73@DFWX10HMPTC01.AMER.DELL.COM>
 Looks like we have Teleworkers in AZ and NV impacted by this.   All are Cox Cable customers, and we saw the issue start for the users about 11AM ET today.  Cox has confirmed an outage and is looking at a "fiber cut" as root cause.   (They aren't providing anything definitive yet).
 -----Original Message-----
 From: outages-bounces at outages.org [mailto:outages-bounces at outages.org] On Behalf Of Vinny_Abello at Dell.com
 Sent: Thursday, April 19, 2012 4:06 PM
 There is a large fiber cut in Los Angeles affecting many providers at this time that's been going on since earlier today. I'm also being affected by this between LA and Dallas and seeing latency on other networks in the LA area as a result of congestion due to decreased capacity.
 -Vinny
 -----Original Message-----
 From: outages-bounces at outages.org [mailto:outages-bounces at outages.org] On Behalf Of Anthony Ledesma
 Sent: Thursday, April 19, 2012 3:52 PM
 We have received a report from Zuora stating the following:
 --
 Anthony Ledesma | FireHost
 From: outages-bounces at outages.org [mailto:outages-bounces at outages.org] On Behalf Of Martin Hepworth
 Sent: Thursday, April 19, 2012 12:43 PM
 Anyone got any further info, they are pointing the finger at their providers?
 --?
 Martin
 -- 
 -- 
 Martin Hepworth
 Oxford, UK
 _______________________________________________
 Outages mailing list
 Outages at outages.org
 https://puck.nether.net/mailman/listinfo/outages
 _______________________________________________
 Outages mailing list
 Outages at outages.org
 https://puck.nether.net/mailman/listinfo/outages
 **********************************************************************
 This e-mail message and any attachments contain confidential information from Express Scripts Holding Company. If you are not the intended recipient, you are hereby notified that disclosure, printing, copying, distribution, or the taking of any action in reliance on the contents of this electronic information is strictly prohibited. If you have received this e-mail message in error, please immediately notify the sender by reply message and then delete the electronic message and any attachments.
  <66DB42B5BE615F4EB8E5E9A88CEBA9FA0603D32A73@DFW1MBX07.mex07a.mlsrvr.com>
  <FD9B2CB2B33E394FAE3B74669547605712B7EA73@DFWX10HMPTC01.AMER.DELL.COM>
 When did LA<->Dallas become part of the Bay Area?
 Owen
 On Apr 19, 2012, at 1:06 PM, <Vinny_Abello at dell.com> wrote:
  <66DB42B5BE615F4EB8E5E9A88CEBA9FA0603D32A73@DFW1MBX07.mex07a.mlsrvr.com>
  <FD9B2CB2B33E394FAE3B74669547605712B7EA73@DFWX10HMPTC01.AMER.DELL.COM>
  <870C3552-95F8-43EF-976D-3197DBE5FB87@delong.com>
 Looking at another report here looks like a fair amount if the western usa
 is affected by this
 -- 
 Martin
 On Thursday, 19 April 2012, Owen DeLong wrote:
 -- 
 -- 
 Martin Hepworth
 Oxford, UK
 -------------- next part --------------
 An HTML attachment was scrubbed...
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120419/c1e74184/attachment.html>
  <66DB42B5BE615F4EB8E5E9A88CEBA9FA0603D32A73@DFW1MBX07.mex07a.mlsrvr.com>
  <FD9B2CB2B33E394FAE3B74669547605712B7EA73@DFWX10HMPTC01.AMER.DELL.COM>
  <870C3552-95F8-43EF-976D-3197DBE5FB87@delong.com>
  <CAGDKorJBRRif64dGRmUikB+H5wPfm-OGh+uAP1y-VFWhn2g39g@mail.gmail.com>
 Sounds like based on time, this could have been a cause of the major cBeyond issues this morning too.
 Blake Pfankuch
 From: outages-bounces at outages.org [mailto:outages-bounces at outages.org] On Behalf Of Martin Hepworth
 Sent: Thursday, April 19, 2012 2:27 PM
 Cc: outages at outages.org
 Looking at another report here looks like a fair amount if the western usa is affected by this
 --
 Martin
 On Thursday, 19 April 2012, Owen DeLong wrote:
 When did LA<->Dallas become part of the Bay Area?
 Owen
 On Apr 19, 2012, at 1:06 PM, <Vinny_Abello at dell.com<javascript:;>> wrote:
 --
 --
 Martin Hepworth
 Oxford, UK
 -------------- next part --------------
 An HTML attachment was scrubbed...
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120419/a58501d8/attachment-0001.html>
  <66DB42B5BE615F4EB8E5E9A88CEBA9FA0603D32A73@DFW1MBX07.mex07a.mlsrvr.com>
  <FD9B2CB2B33E394FAE3B74669547605712B7EA73@DFWX10HMPTC01.AMER.DELL.COM>
  <870C3552-95F8-43EF-976D-3197DBE5FB87@delong.com>
 On 4/19/12 1:16 PM, Owen DeLong wrote:
 Possibly when a protect path starts being used.
 ~Seth
  <66DB42B5BE615F4EB8E5E9A88CEBA9FA0603D32A73@DFW1MBX07.mex07a.mlsrvr.com>
  <FD9B2CB2B33E394FAE3B74669547605712B7EA73@DFWX10HMPTC01.AMER.DELL.COM>
  <870C3552-95F8-43EF-976D-3197DBE5FB87@delong.com>
  <CAGDKorJBRRif64dGRmUikB+H5wPfm-OGh+uAP1y-VFWhn2g39g@mail.gmail.com>
  <FFBD8D4174CD6C4A9AEF17B770644272481A35D8@cpmail1.cpgreeley.com>
 partial fiber cut at approximately 42 km from the Tustin, CA
 site.   It has been reported that the partial cut appears to have been caused by a
 billboard crew who was boring in the area.  The splicing crew is onsite still awaiting the arrival of the digging equipment;
 however, no ETA was available at this time
 Thanks!
 Mike
 From: outages-bounces at outages.org [mailto:outages-bounces at outages.org] On Behalf Of Blake Pfankuch
 Sent: Thursday, April 19, 2012 16:32
 Cc: outages at outages.org
 Sounds like based on time, this could have been a cause of the major cBeyond issues this morning too.
 Blake Pfankuch
 From: outages-bounces at outages.org<mailto:outages-bounces at outages.org> [mailto:outages-bounces at outages.org]<mailto:[mailto:outages-bounces at outages.org]> On Behalf Of Martin Hepworth
 Sent: Thursday, April 19, 2012 2:27 PM
 Cc: outages at outages.org<mailto:outages at outages.org>
 Looking at another report here looks like a fair amount if the western usa is affected by this
 --
 Martin
 On Thursday, 19 April 2012, Owen DeLong wrote:
 When did LA<->Dallas become part of the Bay Area?
 Owen
 On Apr 19, 2012, at 1:06 PM, <Vinny_Abello at dell.com<javascript:;>> wrote:
 --
 --
 Martin Hepworth
 Oxford, UK
 -------------- next part --------------
 An HTML attachment was scrubbed...
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120419/8402e94f/attachment.html>
  <66DB42B5BE615F4EB8E5E9A88CEBA9FA0603D32A73@DFW1MBX07.mex07a.mlsrvr.com>
  <FD9B2CB2B33E394FAE3B74669547605712B7EA73@DFWX10HMPTC01.AMER.DELL.COM>
  <870C3552-95F8-43EF-976D-3197DBE5FB87@delong.com>
  <4F9077A3.2020306@rollernet.us>
 On Thu, Apr 19, 2012 at 01:37:55PM -0700, Seth Mattinen wrote:
 That wouldn't explain why things are *still* down.  The whole idea
 behind a protected path is that it's a failover path; primary path goes
 down, protected path takes over.  Depending on the medium there's a few
 milliseconds of downtime (talking about SONET rings here).  So this
 would make me ask: why are things still down?
 If a protected path was already in use (re: fibre cut), then that would
 make me ask: why hasn't anyone failed back to the primary path?
 I'm inclined to think there's a single carrier involved here, and
 possibly who lacks redundancy (i.e. both paths are shoved through the
 same conduit/fibre bundle which has been cut).  There's no point in
 bothering with redundancy if there's a SPoF anywhere within the
 topology.
 But like everything else, this is all speculation.
 -- 
 | Jeremy Chadwick                                 jdc at parodius.com |
 | Parodius Networking                     http://www.parodius.com/ |
 | UNIX Systems Administrator                 Mountain View, CA, US |
 | Making life hard for others since 1977.             PGP 4BD6C0CB |
 Martin Hepworth wrote:
 Looks like the outage I had at home was related to this. Times are 
 consistent with what I noticed. I could ping their gateway so it was 
 clear this wasn't just a "modem needs reboot":
 http://www.cruzio.com/index.php?option=com_content&task=blogcategory&id=108&Itemid=388
 "As for Wednesday 4/18 at 10:40PM, some of our DSL customers are part of 
 an internet outage and will not be able to get online. We are working 
 with our upstream provider to get this resolved as soon as possible. We 
 do not have an ETA on when this will be fixed.
 UPDATE: as of 1:09am on 4/19 this issue has been resolved by our 
 upstream provider.
 UPDATE: as of 7am on 4/19 several of our DSL customers are still 
 reporting they can't get online. We are working with our upstream 
 provider to get this resolved, and do not have an ETA yet.
 UPDATE: as of 9am on 4/19 this issue appears to have been resolved"
 -- 
 Earthquake Magnitude: 5.2
 Date: Friday, April 20, 2012 19:34:05 UTC
 Location: off the west coast of northern Sumatra
 Latitude: 3.2974; Longitude: 93.8191
 Depth: 11.80 km
  <66DB42B5BE615F4EB8E5E9A88CEBA9FA0603D32A73@DFW1MBX07.mex07a.mlsrvr.com>
  <FD9B2CB2B33E394FAE3B74669547605712B7EA73@DFWX10HMPTC01.AMER.DELL.COM>
  <870C3552-95F8-43EF-976D-3197DBE5FB87@delong.com>
  <CAGDKorJBRRif64dGRmUikB+H5wPfm-OGh+uAP1y-VFWhn2g39g@mail.gmail.com>
  <FFBD8D4174CD6C4A9AEF17B770644272481A35D8@cpmail1.cpgreeley.com>
  <FCCAC6B6142A6045BD7DEA2DDF51B29DB3CAF8@IU-MSSG-MBX108.ads.iu.edu>
 Here is what I see on Level 3's ticketing system...
 Network Event Summary: A partial fiber cut in Tustin, CA impacted transport unprotected, IP and Voice traffic. 911 calls were not impacted.
 Event Ticket ID: 5539336
 Market Area Affected: Tustin, CA
 Ticket Create Date: 4/19/12 3:22:48 PM GMT
 Impacted For: 12 hours 52 minutes
 Event Status: Restored
 Resolve Date: 4/20/12 3:55:14 PM GMT
 Time Since Last Update: 15 hours 5 minutes
 4/20/12 6:00:36 AM GMT  Technical service center confirm all customer services have been restored.
 4/20/12 5:30:05 AM GMT  Transport NOC has completed system roll and all work has been completed. Technical service center is confirming service restoral at this time.
 4/20/12 5:06:37 AM GMT  The fiber repair crew has advised they have closed all tubes as splicing is completed. Transport NOC is seeing some alarms on systems they report are operating outside of span. The Transport NOC is working with the field technician to roll those systems back to original fibers now that they have completed splicing. Technical service center is awaiting Transport NOC to complete roll before they can verify service restoral with the customers.
 4/20/12 4:06:23 AM GMT  The fiber repair team report all splicing has been completed. Technical service center is confirming service restoral with the customer at this time.
 4/20/12 3:42:14 AM GMT  The fiber repair team advised that splicing thru 132 is now complete and they only have two more tubes remaining to splice.
 4/20/12 3:02:55 AM GMT  The fiber repair team advised that splicing in the south pit is complete. The north pit is complete thru 72 and crews are working on 73 thru 96 at this time.
 4/20/12 2:07:47 AM GMT  The fiber repair team has advised a second splice crew has arrived to help expedite the fiber repair.
 4/20/12 1:28:50 AM GMT  The fiber repair team has advised that they have completed splicing on over 48 fibers. The fiber restoration process is ongoing at this time and continues to be on schedule.
 4/20/12 12:38:26 AM GMT  The Transport NOC has advised that the fiber repair team has begun splicing onsite. The Transport NOC has confirmed that the fiber repair team has spliced 24 fibers, which has restored services to some customers. The technicians successfully re-routed traffic and restored services to as many customers as possible. The splicing process is on schedule at this time.
 4/19/12 11:33:04 PM GMT  The fiber repair team has advised that the fiber restoration strategy is to repair the fiber buffer tube by buffer tube in order. The fiber repair team is prepping the fiber for repair at this time and has updated the ETTR from 03:00 GMT to 06:00 GMT. The Transport NOC is investigating the possibility of re-routing customer traffic to expedite service restoration.
 4/19/12 10:31:07 PM GMT  The Transport NOC has advised that some customer whose services were supposed to restore when traffic was rolled to an alternate path have been confirmed as still down. The Transport NOC has advised the fiber repair team to begin the fiber repair to expedite the restoration of services for all of the affected customers. The fiber repair team has provided and ETR of 4 Hours.
 4/19/12 9:26:18 PM GMT  The Transport NOC advised that all systems have been rolled to an alternate path with the exception of some Dark Fiber customers. The Transport NOC advised that the Dark Fiber customers that are up could experience an additional impact to services once the fiber repair commences. The Technical Service Center is compiling a list of Dark Fiber customers that could be impacted so that they can be notified of the situation. The Splicing crew has the damaged fibers excavated and is in the process of placing new cable in the fiber ducks that were not damaged.
 4/19/12 8:10:45 PM GMT  The Transport NOC advised that protected traffic was restored as of 19:18 GMT. The Global Field Service technicians continue to work to roll the remaining impacted traffic to an alternate path. The splicing crew is onsite still awaiting the arrival of the digging equipment; however, no ETA was available at this time.
 4/19/12 7:05:50 PM GMT  The Transport NOC advises that three systems have been rolled and alarms have begun to clear at this time. Global Field Services is in the process of rolling the fourth system and all six systems should be rolled and restored by 20:00 GMT; the Transport NOC continues to monitor alarms. Global Field Services has reported that underground crews are onsite at the cut site and digging equipment should arrive to the site at approximately 19:45 GMT. The Technical Service Center reported that protected customer services are being impacted due to this issue. The Transport NOC confirmed this and has upgraded the event to a transport protected outage.
 4/19/12 6:09:51 PM GMT  The Transport NOC advised that the Global Field Service technicians will commence to roll traffic to an alternate route in approximately 15-minutes. The fiber splicing crew is expected to be onsite at the damage location in approximately 30-minutes. No estimated time to restore is available at this time.
 4/19/12 5:08:34 PM GMT  The Transport NOC advises that splice crews are en-route to the site at this time with an ETA of 17:45 GMT. Global Field Service Technicians are in the process of rolling services to spare fibers as temporary repair and advise that services should be migrated and restored shortly. The Transport NOC is monitoring alarms and will advise when all associated alarms have cleared and services are restored.
 4/19/12 4:39:07 PM GMT  The Transport NOC advises that the Field Technician arrived to the site and shot the fiber, and is seeing a partial fiber cut at approximately 42 km from the Tustin, CA site. Global Field Services advises that construction crews have been engaged and are en-route to the site with an ETA of 17:00 GMT. Splice crews have been engaged to dispatch to the site for investigation; no ETA has is available at this time. A Field Technician has reported that the partial cut appears to have been caused by a billboard crew who was boring in the area. The Transport NOC states that roll options are also being investigated at this time as there are 3 fibers remaining in the 156-count fiber cable to use to roll high-priority services too. No estimated time to complete the service roll has been provided. The Transport NOC confirms that IP and Voice services are also being impacted due to this issue and has upgraded this event to critical level. No estimated time to restore is available.
 4/19/12 3:25:02 PM GMT  The Transport NOC responded to loss of signal alarms, indicating a fiber cut in the Tustin, CA area, which is impacting unprotected traffic. Field Services have been engaged to dispatch to the site to shoot the fibers for further isolation. Estimated time of arrival to the site is 60 minutes.
 Regards,
 Tim
 From: outages-bounces at outages.org [mailto:outages-bounces at outages.org] On Behalf Of Booher, Mike
 Sent: Thursday, April 19, 2012 4:48 PM
 partial fiber cut at approximately 42 km from the Tustin, CA
 site.   It has been reported that the partial cut appears to have been caused by a
 billboard crew who was boring in the area.  The splicing crew is onsite still awaiting the arrival of the digging equipment;
 however, no ETA was available at this time
 Thanks!
 Mike
 From: outages-bounces at outages.org<mailto:outages-bounces at outages.org> [mailto:outages-bounces at outages.org]<mailto:[mailto:outages-bounces at outages.org]> On Behalf Of Blake Pfankuch
 Sent: Thursday, April 19, 2012 16:32
 Cc: outages at outages.org<mailto:outages at outages.org>
 Sounds like based on time, this could have been a cause of the major cBeyond issues this morning too.
 Blake Pfankuch
 From: outages-bounces at outages.org<mailto:outages-bounces at outages.org> [mailto:outages-bounces at outages.org]<mailto:[mailto:outages-bounces at outages.org]> On Behalf Of Martin Hepworth
 Sent: Thursday, April 19, 2012 2:27 PM
 Cc: outages at outages.org<mailto:outages at outages.org>
 Looking at another report here looks like a fair amount if the western usa is affected by this
 --
 Martin
 On Thursday, 19 April 2012, Owen DeLong wrote:
 When did LA<->Dallas become part of the Bay Area?
 Owen
 On Apr 19, 2012, at 1:06 PM, <Vinny_Abello at dell.com<javascript:;>> wrote:
 --
 --
 Martin Hepworth
 Oxford, UK
 -------------- next part --------------
 An HTML attachment was scrubbed...
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120420/6e1043db/attachment-0001.html>
###############################################################
END
###############################################################

###############################################################
fiber cut in Spokane, WA
###############################################################
 Reposting two messages from NANOG.
 -----Original Message-----
 From: John van Oppen [mailto:jvanoppen at spectrumnet.us] 
 Sent: Thursday, April 19, 2012 6:37 PM
 We saw the issues on the AS209 backbone as well from our vantage point here in Seattle but we also show a circuit we have down (that rides Qwest) between Yakima, WA and Spokane, WA.   The outage corresponds to the IP issues so I would think that it is probably the same cut affecting both the wave we are seeing as out and the IP issues (which mostly seem better now).
 Thanks,
 John @ AS11404 here in Seattle.
 -----Original Message-----
 From: Aaron C. de Bruyn [mailto:aaron at heyaaron.com] 
 Sent: Thursday, April 19, 2012 5:41 PM
 Cc: nanog at nanog.org
 On Thu, Apr 19, 2012 at 14:58, Brandon Applegate <brandon at burn.net> wrote:
 On Twitter, LastCenturyLink told me its a fiber cut in Spokane, WA.
 -A
###############################################################
END
###############################################################

###############################################################
Earthlink Business/One Communications issues
###############################################################
 We have a customer site on an Earthlink/One Communications T-1
 (Massachusetts) that dropped off the radar about 11:30 PM EDT.  Can't trace
 past the Savvis or Level(s) borders with Earthlink/One at the moment.
  Nothing but endless music on hold at Earthlink support.  Anyone else
 seeing any issues?
 traceroute 64.179.91.105
 traceroute to 64.179.91.105 (64.179.91.105), 64 hops max, 40 byte packets
  1  174.136.23.2 (174.136.23.2)  0.980 ms  0.845 ms  0.854 ms
  2  206.123.64.78 (206.123.64.78)  0.730 ms  0.798 ms  0.733 ms
  3  * te-4-3.car2.Dallas1.Level3.net (4.79.182.237)  7.186 ms  3.534 ms
  4  vlan70.csw2.Dallas1.Level3.net (4.69.145.126)  8.220 ms  6.171 ms
  0.859 ms
  5  ae-72-72.ebr2.Dallas1.Level3.net (4.69.151.142)  0.982 ms  2.551 ms
  1.480 ms
  6  ae-3-3.ebr2.NewYork1.Level3.net (4.69.137.122)  35.458 ms  35.310 ms
  34.946 ms
  7  ae-82-82.csw3.NewYork1.Level3.net (4.69.148.42)  42.436 ms  43.896 ms
     ae-62-62.csw1.NewYork1.Level3.net (4.69.148.34)  45.196 ms
  8  ae-24-70.car4.NewYork1.Level3.net (4.69.155.70)  35.394 ms
     ae-44-90.car4.NewYork1.Level3.net (4.69.155.198)  35.030 ms  34.903 ms
  9  ONE-COMMUNI.car4.NewYork1.Level3.net (4.53.95.26)  35.330 ms  35.545 ms
  35.322 ms
 10  * * *
 11  * * *
 12  * * *
 traceroute 64.179.91.105
 traceroute to 64.179.91.105 (64.179.91.105), 64 hops max, 40 byte packets
  1  fe-xl-1.core-1.pjstny.gbcx.net (204.89.131.6)  0.627 ms  0.531 ms
  0.533 ms
  2  fe-bge0-br1-mtsnny.gbcx.net (216.168.138.46)  0.868 ms  0.810 ms  0.782
 ms
  3  454a1125.cst.lightpath.net (69.74.17.37)  1.915 ms  4.898 ms  2.392 ms
  4  r2-ge13-2-0.cst.bthpny.cv.net (65.19.105.177)  2.983 ms  2.306 ms
  2.619 ms
  5  41333f95.cst.lightpath.net (65.51.63.149)  3.285 ms  3.110 ms  3.166 ms
  6  64.15.4.18 (64.15.4.18)  3.118 ms  2.913 ms  4.533 ms
  7  64.15.0.194 (64.15.0.194)  4.311 ms 64.15.2.173 (64.15.2.173)  3.231 ms
 64.15.1.90 (64.15.1.90)  5.680 ms
  8  64.15.0.214 (64.15.0.214)  5.079 ms 64.15.1.14 (64.15.1.14)  4.294 ms
 64.15.1.22 (64.15.1.22)  4.723 ms
  9  bcs2-ge-7-0-0.NewYork.savvis.net (208.173.134.73)  4.745 ms  6.113 ms
 166.63.145.113 (166.63.145.113)  4.267 ms
 10  cr2-tengig-0-15-5-0.NewYork.savvis.net (204.70.197.10)  4.469 ms  5.342
 ms  5.496 ms
 11  er1-te-2-1.newyorknyd.savvis.net (204.70.196.70)  4.328 ms  5.462 ms
  4.823 ms
 12  cr2-tengig-0-3-0-0.boston.savvis.net (204.70.198.174)  14.650 ms
  12.116 ms  12.646 ms
 13  cr1-pos-0-0-3-2.Washington.savvis.net (204.70.192.21)  11.080 ms
  11.345 ms  10.163 ms
 14  208.172.49.150 (208.172.49.150)  114.426 ms  198.303 ms  51.570 ms
 15  * * *
 16  * * *
 -------------- next part --------------
 An HTML attachment was scrubbed...
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120420/c875717d/attachment-0001.html>
 Unless its fallout from the LA Dallas issue??
 Martin
 On Friday, 20 April 2012, Drew Linsalata wrote:
 -- 
 -- 
 Martin Hepworth
 Oxford, UK
 -------------- next part --------------
 An HTML attachment was scrubbed...
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120420/0ce8986a/attachment.html>
###############################################################
END
###############################################################

###############################################################
[Outages-discussion] Short interruption to IPv6 version of Cogent's site
###############################################################
 IPv6 access to their website dropped again at 6:57 am Central.  I can
 traceroute6 and ping6 it just fine, it's just HTTP access.	
 In fact, v4 access to www.cogentco.com is down, resulting in "Database
 Error: Unable to connect to the database:Could not connect to MySQL".   The
 v6 version of their site fails to load more quickly. =)
 nagios:~/bin# wget -4 www.cogentco.com
 --2012-04-20 07:20:40--  http://www.cogentco.com/
 Resolving www.cogentco.com... 38.100.128.10
 Connecting to www.cogentco.com|38.100.128.10|:80... connected.
 HTTP request sent, awaiting response... 500 Internal Server Error
 2012-04-20 07:20:49 ERROR 500: Internal Server Error.
 nagios:~/bin# wget -6 www.cogentco.com
 --2012-04-20 07:20:54--  http://www.cogentco.com/
 Resolving www.cogentco.com... 2001:550:1::cc01
 Connecting to www.cogentco.com|2001:550:1::cc01|:80... connected.
 HTTP request sent, awaiting response... 500 Internal Server Error
 2012-04-20 07:20:54 ERROR 500: Internal Server Error.
 nagios:~/bin#
 -----Original Message-----
 From: outages-discussion-bounces at outages.org
 [mailto:outages-discussion-bounces at outages.org] On Behalf Of Frank Bulk
 Sent: Friday, April 20, 2012 6:56 AM
 Subject: [Outages-discussion] Short interruption to IPv6 version of Cogent's
 site
 Did anyone else see IPv6 access to www.cogentco.com and ipv6.cogentco.com
 down from 4 am until 5:27 am Central?
 Perhaps more was down, but I'm just monitoring the v6 side of that site
 today.
 Regards,
 _______________________________________________
 Outages-discussion mailing list
 Outages-discussion at outages.org
 https://puck.nether.net/mailman/listinfo/outages-discussion
 Back up by 8 am Central.
 Frank
 -----Original Message-----
 From: outages-bounces at outages.org [mailto:outages-bounces at outages.org] On
 Behalf Of Frank Bulk
 Sent: Friday, April 20, 2012 7:21 AM
 Subject: Re: [outages] [Outages-discussion] Short interruption to IPv6
 version of Cogent's site
 IPv6 access to their website dropped again at 6:57 am Central.  I can
 traceroute6 and ping6 it just fine, it's just HTTP access.	
 In fact, v4 access to www.cogentco.com is down, resulting in "Database
 Error: Unable to connect to the database:Could not connect to MySQL".   The
 v6 version of their site fails to load more quickly. =)
 nagios:~/bin# wget -4 www.cogentco.com
 --2012-04-20 07:20:40--  http://www.cogentco.com/
 Resolving www.cogentco.com... 38.100.128.10
 Connecting to www.cogentco.com|38.100.128.10|:80... connected.
 HTTP request sent, awaiting response... 500 Internal Server Error
 2012-04-20 07:20:49 ERROR 500: Internal Server Error.
 nagios:~/bin# wget -6 www.cogentco.com
 --2012-04-20 07:20:54--  http://www.cogentco.com/
 Resolving www.cogentco.com... 2001:550:1::cc01
 Connecting to www.cogentco.com|2001:550:1::cc01|:80... connected.
 HTTP request sent, awaiting response... 500 Internal Server Error
 2012-04-20 07:20:54 ERROR 500: Internal Server Error.
 nagios:~/bin#
 -----Original Message-----
 From: outages-discussion-bounces at outages.org
 [mailto:outages-discussion-bounces at outages.org] On Behalf Of Frank Bulk
 Sent: Friday, April 20, 2012 6:56 AM
 Subject: [Outages-discussion] Short interruption to IPv6 version of Cogent's
 site
 Did anyone else see IPv6 access to www.cogentco.com and ipv6.cogentco.com
 down from 4 am until 5:27 am Central?
 Perhaps more was down, but I'm just monitoring the v6 side of that site
 today.
 Regards,
 _______________________________________________
 Outages-discussion mailing list
 Outages-discussion at outages.org
 https://puck.nether.net/mailman/listinfo/outages-discussion
 _______________________________________________
 Outages mailing list
 Outages at outages.org
 https://puck.nether.net/mailman/listinfo/outages
###############################################################
END
###############################################################

###############################################################
Comcast Outage 8am-9am Eastern
###############################################################
 Comcast appears to have had some sort of outage isolating portions of their southeast michigan network from chicago.
 It just recovered itself when I was about to post this note.
 - Jared
###############################################################
END
###############################################################

###############################################################
Qwest ISP issues in Chicago
###############################################################
 Seeing a major outage of Qwest connectivity in Chicago. SIP Trunks and Internet connectivity seem to be affected..
 -Mike
 -- 
 Michael Vallaly <mvallaly at nolatency.com>
 Same thing here, took out our DS-3.  Qwest/CenturyLink is aware of the 
 issue with no ETA.  Saying it's a core routing problem.
 On 4/24/2012 11:02 AM, Michael Vallaly wrote:
 -- 
 ---- ---- ---- ----
 Chris Gotstein, Network Engineer, U.P. Logon/Computer Connection U.P.
 http://uplogon.com | +1 906 774 4847 | chris at uplogon.com
  <4F96D067.4020603@uplogon.com>
 Some routing issue toward AOL from Sunnyvale, California. They spoke of
 core routing issue along with fibercut.
 P.
 --
 Pascal Charest - *Cutting-edge technology consultant*
 On Tue, Apr 24, 2012 at 12:10 PM, Chris Gotstein <chris at uplogon.com> wrote:
 -------------- next part --------------
 An HTML attachment was scrubbed...
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120424/06d8507e/attachment.html>
  <4F96D067.4020603@uplogon.com>
  <CAFY7MFdXOTdD8vvz5qDAV4cfQ4B6Wpj4tbgaaq4j9bte2STgtA@mail.gmail.com>
 And we now have:
 [...]
 http://www.nbcsandiego.com/news/local/Broadband-Lines-Cut-Copper-Theft-Alpine-Defense-Dept-148669775.html
 P.
 --
 Pascal Charest - *Cutting-edge technology consultant*
 On Tue, Apr 24, 2012 at 1:15 PM, Pascal Charest <
 pascal.charest at labsphoenix.com> wrote:
 >>>
 >>> Seeing a major outage of Qwest connectivity in Chicago. SIP Trunks and
 >>> Internet connectivity seem to be affected..
 >>>
 >>> -Mike
 >>>
 >>>
 -------------- next part --------------
 An HTML attachment was scrubbed...
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120424/e8bb3d2d/attachment.html>
  <4F96D067.4020603@uplogon.com>
  <CAFY7MFdXOTdD8vvz5qDAV4cfQ4B6Wpj4tbgaaq4j9bte2STgtA@mail.gmail.com>
  <CAFY7MFcCENifbf=-hX9TetyQGj8_tdtbRL-CHnky_e8bTXkpQQ@mail.gmail.com>
 Is anyone still dealing with this outage?  My office uses Qwest as its ISP
 and we're still having issues to some sites. Specifically, Google's auth
 server, accounts.google.com.
 http://pastebin.com/AprVPJ5W
 We seem to be unable to get past 64.233.174.204 or 64.233.174.206.  I was
 in touch w/ Qwest/Centurylink support this morning and they're saying that
 things are all patched up on their end and that the fault lies with some
 other carriers.
 -------------- next part --------------
 An HTML attachment was scrubbed...
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120425/2d6b9250/attachment.html>
###############################################################
END
###############################################################

###############################################################
Integra Outage Northern Colorado
###############################################################
 Seeing issues across a lot of our customers on Integra, including a branch office of ours.  Anyone know what's up?  We have a ticket open, but getting the usual sludge and no ETA.
 Blake Pfankuch
 -------------- next part --------------
 An HTML attachment was scrubbed...
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120424/2b6fc620/attachment.html>
###############################################################
END
###############################################################

###############################################################
USAD SIP Dropped Calls
###############################################################
 We are receiving customer complaints of dropped calls on our USAD SIP
 trunks. We are also noticing intermittent packet loss and high variability
 in latency (according to ping/traceroute). Is anyone else experiencing
 problems?
 dwhite at quark:~$ dig +short -x 66.116.118.50
 cust-66.116.118.50.switchnap.com.
 dwhite at quark:~$ ping -A -q -c 50 66.116.118.50
 PING 66.116.118.50 (66.116.118.50) 56(84) bytes of data.
 --- 66.116.118.50 ping statistics ---
 50 packets transmitted, 43 received, 14% packet loss, time 9992ms
 rtt min/avg/max/mdev = 46.929/153.021/512.515/119.183 ms, pipe 3, ipg/ewma
 203.936/104.808 ms
 dwhite at quark:~$ mtr --report -c 50 66.116.118.50
 HOST: quark                       Loss%   Snt   Last   Avg  Best  Wrst StDev
    1.|-- 10.0.2.1                   0.0%    50    0.6   1.8   0.5   3.6 0.8
    2.|-- olp-67-217-152-5.olp.net   0.0%    50    1.4   1.8   0.7  10.1 1.3
    3.|-- router.olp.net             0.0%    50    0.9   0.9   0.7   4.1 0.5
    4.|-- olp-67-217-152-34.olp.net  0.0%    50    1.2   7.0   1.2 277.4 39.0
    5.|-- ae5-694.edge9.dallas1.lev  0.0%    50    8.2   8.6   8.1  23.8 2.3
    6.|-- vlan80.csw3.dallas1.level  0.0%    50    8.1  11.8   8.1  20.5 4.2
    7.|-- ae-82-82.ebr2.dallas1.lev  0.0%    50    8.2   8.7   8.1  17.6 1.5
    8.|-- ae-2-2.ebr1.denver1.level  0.0%    50   22.6  25.8  22.5  34.3 4.3
    9.|-- ae-1-6.bar2.lasvegas1.lev  0.0%    50   41.8  44.9  41.8  95.8 10.9
   10.|-- ae-5-5.car2.lasvegas1.lev  0.0%    50   41.9  57.4  41.9 192.5 35.5
   11.|-- switch-comm.car2.lasvegas  0.0%    50   42.7  42.8  42.6  43.1 0.1
   12.|-- te2-6.las-core2-1.switchn  0.0%    50   42.9  51.6  42.6 239.4 35.7
   13.|-- te4-1.las-esw09.switchnap  2.0%    50   43.2  43.9  42.9  59.0 3.0
   14.|-- cust-66.209.71.30.switchn 34.0%    50  142.4 381.9  99.8 1371.  283.3
   15.|-- cust-66.116.118.50.switch 64.0%    50  192.6 327.1 108.7 726.2 208.8
 - Dan
 >From what locations are you experiencing the problems?
 Thanks
 Neal
 We are receiving customer complaints of dropped calls on our USAD SIP
 trunks. We are also noticing intermittent packet loss and high variability
 in latency (according to ping/traceroute). Is anyone else experiencing
 problems?
 dwhite at quark:~$ dig +short -x 66.116.118.50
 cust-66.116.118.50.switchnap.com.
 dwhite at quark:~$ ping -A -q -c 50 66.116.118.50
 PING 66.116.118.50 (66.116.118.50) 56(84) bytes of data.
 --- 66.116.118.50 ping statistics ---
 50 packets transmitted, 43 received, 14% packet loss, time 9992ms
 rtt min/avg/max/mdev = 46.929/153.021/512.515/119.183 ms, pipe 3, ipg/ewma
 203.936/104.808 ms
 dwhite at quark:~$ mtr --report -c 50 66.116.118.50
 HOST: quark                       Loss%   Snt   Last   Avg  Best  Wrst
 StDev
    1.|-- 10.0.2.1                   0.0%    50    0.6   1.8   0.5   3.6 0.8
    2.|-- olp-67-217-152-5.olp.net   0.0%    50    1.4   1.8   0.7  10.1 1.3
    3.|-- router.olp.net             0.0%    50    0.9   0.9   0.7   4.1 0.5
    4.|-- olp-67-217-152-34.olp.net  0.0%    50    1.2   7.0   1.2 277.4
 39.0
    5.|-- ae5-694.edge9.dallas1.lev  0.0%    50    8.2   8.6   8.1  23.8 2.3
    6.|-- vlan80.csw3.dallas1.level  0.0%    50    8.1  11.8   8.1  20.5 4.2
    7.|-- ae-82-82.ebr2.dallas1.lev  0.0%    50    8.2   8.7   8.1  17.6 1.5
    8.|-- ae-2-2.ebr1.denver1.level  0.0%    50   22.6  25.8  22.5  34.3 4.3
    9.|-- ae-1-6.bar2.lasvegas1.lev  0.0%    50   41.8  44.9  41.8  95.8
 10.9
   10.|-- ae-5-5.car2.lasvegas1.lev  0.0%    50   41.9  57.4  41.9 192.5
 35.5
   11.|-- switch-comm.car2.lasvegas  0.0%    50   42.7  42.8  42.6  43.1 0.1
   12.|-- te2-6.las-core2-1.switchn  0.0%    50   42.9  51.6  42.6 239.4
 35.7
   13.|-- te4-1.las-esw09.switchnap  2.0%    50   43.2  43.9  42.9  59.0 3.0
   14.|-- cust-66.209.71.30.switchn 34.0%    50  142.4 381.9  99.8 1371.
 283.3
   15.|-- cust-66.116.118.50.switch 64.0%    50  192.6 327.1 108.7 726.2
 208.8
 - Dan
 _______________________________________________
 Outages mailing list
 Outages at outages.org
 https://puck.nether.net/mailman/listinfo/outages
  <OF052AD24A.3EBFF956-ON882579EB.0070E47A-882579EB.0070FC5C@ahm.am.honda.com>
 Tulsa, Oklahoma.
 I don't know, geographically, where USA Digital is located, or at which POP
 we terminate.
 On 04/25/12?13:34?-0700, Cornelius_Lewis at ahm.honda.com wrote:
 >From what locations are you experiencing the problems?
 >Thanks
 >Neal
 >From:	Dan White <dwhite at olp.net>
 >To:	outages at outages.org
 >Date:	04/25/2012 01:28 PM
 >Subject:	[outages] USAD SIP Dropped Calls
 >Sent by:	outages-bounces at outages.org
 >We are receiving customer complaints of dropped calls on our USAD SIP
 >trunks. We are also noticing intermittent packet loss and high variability
 >in latency (according to ping/traceroute). Is anyone else experiencing
 >problems?
 >dwhite at quark:~$ dig +short -x 66.116.118.50
 >cust-66.116.118.50.switchnap.com.
 >dwhite at quark:~$ ping -A -q -c 50 66.116.118.50
 >PING 66.116.118.50 (66.116.118.50) 56(84) bytes of data.
 >--- 66.116.118.50 ping statistics ---
 >50 packets transmitted, 43 received, 14% packet loss, time 9992ms
 >rtt min/avg/max/mdev = 46.929/153.021/512.515/119.183 ms, pipe 3, ipg/ewma
 >203.936/104.808 ms
 >dwhite at quark:~$ mtr --report -c 50 66.116.118.50
 >HOST: quark                       Loss%   Snt   Last   Avg  Best  Wrst
 >StDev
 >39.0
 >10.9
 >35.5
 >35.7
 >283.3
 >208.8
  <OF052AD24A.3EBFF956-ON882579EB.0070E47A-882579EB.0070FC5C@ahm.am.honda.com>
  <20120425203618.GL4595@dan.olp.net>
 Hmm, SwitchNAP has one location-- in Las Vegas.
 -- Corey
 On Apr 25, 2012, at 1:36 PM, Dan White wrote:
 -------------- next part --------------
 A non-text attachment was scrubbed...
 Name: signature.asc
 Type: application/pgp-signature
 Size: 487 bytes
 Desc: Message signed with OpenPGP using GPGMail
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120425/51e7e9fb/attachment.sig>
  <OF052AD24A.3EBFF956-ON882579EB.0070E47A-882579EB.0070FC5C@ahm.am.honda.com>
  <20120425203618.GL4595@dan.olp.net>
  <94E0E7DD-0804-4BC3-8694-B16CC76957F4@sequestered.net>
 To be entirely accurate, they have 7 locations. All in LAS. There are two networks for "house" pipe. I'd call one "cheap" and the other "good" blends of transit. 
 On Apr 25, 2012, at 13:37, Corey Quinn <corey at sequestered.net> wrote:
 >>> From what locations are you experiencing the problems?
 >>> 
 >>> Thanks
 >>> 
 >>> Neal
 >>> 
 >>> 
 >>> 
 >>> 
 >>> From:    Dan White <dwhite at olp.net>
 >>> To:    outages at outages.org
 >>> Date:    04/25/2012 01:28 PM
 >>> Subject:    [outages] USAD SIP Dropped Calls
 >>> Sent by:    outages-bounces at outages.org
 >>> 
 >>> 
 >>> 
 >>> We are receiving customer complaints of dropped calls on our USAD SIP
 >>> trunks. We are also noticing intermittent packet loss and high variability
 >>> in latency (according to ping/traceroute). Is anyone else experiencing
 >>> problems?
 >>> 
 >>> dwhite at quark:~$ dig +short -x 66.116.118.50
 >>> cust-66.116.118.50.switchnap.com.
 >>> dwhite at quark:~$ ping -A -q -c 50 66.116.118.50
 >>> PING 66.116.118.50 (66.116.118.50) 56(84) bytes of data.
 >>> 
 >>> --- 66.116.118.50 ping statistics ---
 >>> 50 packets transmitted, 43 received, 14% packet loss, time 9992ms
 >>> rtt min/avg/max/mdev = 46.929/153.021/512.515/119.183 ms, pipe 3, ipg/ewma
 >>> 203.936/104.808 ms
 >>> 
 >>> dwhite at quark:~$ mtr --report -c 50 66.116.118.50
 >>> HOST: quark                       Loss%   Snt   Last   Avg  Best  Wrst
 >>> StDev
 >>> 1.|-- 10.0.2.1                   0.0%    50    0.6   1.8   0.5   3.6 0.8
 >>> 2.|-- olp-67-217-152-5.olp.net   0.0%    50    1.4   1.8   0.7  10.1 1.3
 >>> 3.|-- router.olp.net             0.0%    50    0.9   0.9   0.7   4.1 0.5
 >>> 4.|-- olp-67-217-152-34.olp.net  0.0%    50    1.2   7.0   1.2 277.4
 >>> 39.0
 >>> 5.|-- ae5-694.edge9.dallas1.lev  0.0%    50    8.2   8.6   8.1  23.8 2.3
 >>> 6.|-- vlan80.csw3.dallas1.level  0.0%    50    8.1  11.8   8.1  20.5 4.2
 >>> 7.|-- ae-82-82.ebr2.dallas1.lev  0.0%    50    8.2   8.7   8.1  17.6 1.5
 >>> 8.|-- ae-2-2.ebr1.denver1.level  0.0%    50   22.6  25.8  22.5  34.3 4.3
 >>> 9.|-- ae-1-6.bar2.lasvegas1.lev  0.0%    50   41.8  44.9  41.8  95.8
 >>> 10.9
 >>> 10.|-- ae-5-5.car2.lasvegas1.lev  0.0%    50   41.9  57.4  41.9 192.5
 >>> 35.5
 >>> 11.|-- switch-comm.car2.lasvegas  0.0%    50   42.7  42.8  42.6  43.1 0.1
 >>> 12.|-- te2-6.las-core2-1.switchn  0.0%    50   42.9  51.6  42.6 239.4
 >>> 35.7
 >>> 13.|-- te4-1.las-esw09.switchnap  2.0%    50   43.2  43.9  42.9  59.0 3.0
 >>> 14.|-- cust-66.209.71.30.switchn 34.0%    50  142.4 381.9  99.8 1371.
 >>> 283.3
 >>> 15.|-- cust-66.116.118.50.switch 64.0%    50  192.6 327.1 108.7 726.2
 >>> 208.8
###############################################################
END
###############################################################

###############################################################
CenturyLink Issues in Atlanta
###############################################################
 All:
 We are seeing some routing issues with our CenturyLink connection in
 Atlanta. If I am our peering router with CL, a traceroute sourced from
 one interface works while another fails.
 // works
   2 63.233.83.253 0 msec 0 msec 4 msec
   3 67.14.8.86 16 msec 16 msec 16 msec
   4 63.148.124.242 [AS 209] 32 msec 36 msec 32 msec
 // fails
   2 63.233.83.253 0 msec 4 msec 0 msec
   3  *  *  *
 Is anyone else experiencing issues with CenturyLink? We have a ticket
 opened but there are no updates at this time.
 -- 
 Devon
 Our colo in Atlanta (Peak10) had to disconnect from CenturyLink due to this
 same issue. We're routing to them via XO.
 --
 Esteban Santana Santana
 "When life hands you lemons, ask for tequila and salt."
 -Anonymous
 On Thu, Apr 26, 2012 at 9:12 AM, Devon True <devon at noved.org> wrote:
 -------------- next part --------------
 An HTML attachment was scrubbed...
 URL: <https://puck.nether.net/pipermail/outages/attachments/20120426/834a863c/attachment-0001.html>
  <CANaEE7KqHD7B53VMm09WrmZnYs6uHM=dYmWJGp2X=Pqwt+efZQ@mail.gmail.com>
 Once upon a time, Esteban Santana Santana <mentalpower at gmail.com> said:
 CenturyLink has a number of different networks, so it would be helpful
 if people would identify the AS they are talking about when they say
 "CenturyLink".
 -- 
 Chris Adams <cmadams at hiwaay.net>
 Systems and Network Administrator - HiWAAY Internet Services
 I don't speak for anybody but myself - that's enough trouble.
  <CANaEE7KqHD7B53VMm09WrmZnYs6uHM=dYmWJGp2X=Pqwt+efZQ@mail.gmail.com>
  <20120426144625.GA3887@hiwaay.net>
 On 26-Apr-12 10:46 AM, Chris Adams wrote:
 AS 209.
 -- 
 Devon
  <CANaEE7KqHD7B53VMm09WrmZnYs6uHM=dYmWJGp2X=Pqwt+efZQ@mail.gmail.com>
  <20120426144625.GA3887@hiwaay.net> <4F9962E3.7030109@noved.org>
 We are seeing intermittent routing issues with AS209 also.
 -----Original Message-----
 From: outages-bounces at outages.org [mailto:outages-bounces at outages.org] On Behalf Of Devon True
 Sent: Thursday, April 26, 2012 10:00 AM
 On 26-Apr-12 10:46 AM, Chris Adams wrote:
 AS 209.
 --
 Devon
 _______________________________________________
 Outages mailing list
 Outages at outages.org
 https://puck.nether.net/mailman/listinfo/outages
 ________________________________
 DISCLAIMER: The information in this message is considered confidential and may be legally privileged. It is intended solely for the addressee. Access to this message by anyone else is unauthorized. If you are not the intended recipient, any disclosure, copying, or distribution of the message, or any action or omission taken by you in reliance on it, is prohibited and may be unlawful. Please immediately contact the sender if you have received this message in error.
###############################################################
END
###############################################################

###############################################################
above.net / he.net routing problems in LAX?
###############################################################
 We're seeing major routing issues in Los Angeles with peering from 
 HE.net <-> Above.net, specifically after mpr1.lax2.us.above.net.
 Above.net's looking glass in LA shows traces dying right after he.net,
   1  xe-0-1-0.cr2.lax112.us.above.net (64.125.30.250)  0.780 ms  0.571 
 ms  0.479 ms
   2  xe-8-0-0.mpr1.lax12.us.above.net (64.125.30.30)  0.393 ms  0.483 ms 
   0.404 ms
   3  10gigabitethernet1-3.core1.lax1.he.net (206.223.123.37)  0.884 ms 
 4.619 ms  1.100 ms
   4  * * *
 Both sides seem to be blaming the other.  Is anyone routing via 
 above.net or he.net seeing similar issues out west?
 Best Regards,
 Erik Soroka
 Tier III System Administrator
 InMotion Hosting, Inc.
 eriks at inmotionhosting.com
  <CA+w5D_Dk3MqKADku9nGjwmmjM-ZKBwVwUSU7FOXJ5zBxFAV0BA@mail.gmail.com>
  <4F99D842.8020809@he.net> <4F99DAC6.40306@inmotionhosting.com>
  <4F99DCE1.3010702@he.net>
 Rob,
 Thanks for getting back to us.  You are indeed correct, the problem does 
 appear to be inside the Corporate Colocation network.  Their NOC has 
 been insisting this was upstream and the routing issues we're seeing as 
 a result incorrectly implicated peers along the way.  I apologize for 
 wasting your time, and again thank you for troubleshooting and responses.
 -Erik
 On 04/26/2012 07:40 PM, Rob Mosher wrote:
 >>> Hi Erik,
 >>>
 >>> Where did you get the information that we were blaming above.net for
 >>> anything? We are not aware of any issue between above.net and he.net,
 >>> nor have we seen any tickets opened regarding this. Can you please
 >>> include the source and destination IP addresses of the traceroute quoted
 >>> below, as well as a traceroute in the opposite direction if possible?
 >>>
 >>> As far as I can tell, everything looks clean here.
 >>>
 >>> root at tserv15:~# tcptraceroute lg.above.net
 >>> traceroute to lg.above.net (64.124.253.67), 30 hops max, 60 byte packets
 >>> 1 gige-g4-6.core1.lax1.he.net (66.220.18.41) 7.683 ms 7.725 ms
 >>> 7.821 ms
 >>> 2 mpr1.lax2.us.above.net (206.223.123.71) 0.781 ms 0.766 ms 0.767 ms
 >>> 3 xe-3-0-0.cr1.lax112.us.above.net (64.125.30.17) 1.055 ms 1.057
 >>> ms 1.293 ms
 >>> 4 xe-2-1-0.cr1.sjc2.us.above.net (64.125.24.18) 10.125 ms 10.129
 >>> ms 10.120 ms
 >>> 5 xe-1-1-0.er1.sjc2.us.above.net (64.125.26.198) 9.815 ms 9.800 ms
 >>> 9.800 ms
 >>> 6 f1-1.itr3.sjc2.us.corp.above.net (64.124.248.122) 10.239 ms
 >>> 10.002 ms 9.995 ms
 >>> 7 lg.above.net (64.124.253.67) 10.227 ms 10.338 ms 10.267 ms
 >>>
 >>> --
 >>> Rob Mosher
 >>> Senior Network and Software Engineer
 >>> Hurricane Electric / AS6939
 >>>
 >>>
 >>>> ---------- Forwarded message ----------
 >>>> From: "Erik S." <eriks at inmotionhosting.com
 >>>> <mailto:eriks at inmotionhosting.com>>
 >>>> Date: Apr 26, 2012 6:06 PM
 >>>> Subject: [outages] above.net <http://above.net> / he.net
 >>>> <http://he.net> routing problems in LAX?
 >>>> To: "outages at outages.org <mailto:outages at outages.org>"
 >>>> <outages at outages.org <mailto:outages at outages.org>>
 >>>>
 >>>> We're seeing major routing issues in Los Angeles with peering from
 >>>> HE.net <-> Above.net, specifically after mpr1.lax2.us.above.net
 >>>> <http://mpr1.lax2.us.above.net>.
 >>>>
 >>>> Above.net's looking glass in LA shows traces dying right after he.net
 >>>> <http://he.net>,
 >>>>
 >>>> 1 xe-0-1-0.cr2.lax112.us.above.net
 >>>> <http://xe-0-1-0.cr2.lax112.us.above.net> (64.125.30.250
 >>>> <tel:%2864.125.30.250>) 0.780 ms 0.571 ms 0.479 ms
 >>>> 2 xe-8-0-0.mpr1.lax12.us.above.net
 >>>> <http://xe-8-0-0.mpr1.lax12.us.above.net> (64.125.30.30) 0.393 ms
 >>>> 0.483 ms 0.404 ms
 >>>> 3 10gigabitethernet1-3.core1.lax1.he.net
 >>>> <http://10gigabitethernet1-3.core1.lax1.he.net> (206.223.123.37)
 >>>> 0.884 ms 4.619 ms 1.100 ms
 >>>> 4 * * *
 >>>>
 >>>> Both sides seem to be blaming the other. Is anyone routing via
 >>>> above.net <http://above.net> or he.net <http://he.net> seeing similar
 >>>> issues out west?
 >>>>
 >>>> Best Regards,
 >>>>
 >>>> Erik Soroka
 >>>> Tier III System Administrator
 >>>> InMotion Hosting, Inc.
 >>>> eriks at inmotionhosting.com <mailto:eriks at inmotionhosting.com>
 >>>>
 >>>>
 >>>> _______________________________________________
 >>>> Outages mailing list
 >>>> Outages at outages.org <mailto:Outages at outages.org>
 >>>> https://puck.nether.net/mailman/listinfo/outages
 >>>>
###############################################################
END
###############################################################

###############################################################
 above.net / he.net routing problems in LAX?
###############################################################
  <CA+w5D_Dk3MqKADku9nGjwmmjM-ZKBwVwUSU7FOXJ5zBxFAV0BA@mail.gmail.com>
  <4F99D842.8020809@he.net>
 Hi Rob,
 Thanks for the quick response.  Sorry for the confusion, the information 
 I was provided at first was a little cloudy.  We also use GT-T/Packet 
 Exchange and they're pointing this at above.net.
 Here's the response we received from PKX/GT-T regarding the routing 
 anomaly we're experiencing:
 -------- Original Message --------
 Subject: GTT TT#(49181), SVC ID: PEX/NATIVE LAYER 3/9014-1 RE: Routing 
 issue preventing our network from reaching *
 Date: Thu, 26 Apr 2012 21:52:27 +0000
 From: GTT NOC <NOC at gt-t.net>
  From LAX core device the routes goes to abovenet for both the ip 
 address and dies within the above net network.
 We will investigate with abovenet and update you shortly.
 cr1.lax1#traceroute 108.171.183.75
 Type Ctrl-C to abort.
 -------------------------------------------------------------------
 Tracing the route to 108.171.183.75, 30 hops max, 40 byte packets
 -------------------------------------------------------------------
   TTL Hostname            Probe1      Probe2      Probe3
    1  mpr1.lax2.us.above.net (206.223.123.71) 012.000 ms  001.000 ms 
 001.000 ms
    2                        *           *           *
    3                        *           *           *
    4                        *           *           *
    5                        *
 -- End forwarded message --
 If they have not yet opened a ticket with you, this leads me to believe 
 there's a problem elsewhere and we're being given the run-around.
 The particular trace above is to one of our off-site boxes at rackspace.
 This is an issue that seems to be impacting multiple networks and 
 regions, so I'm not entirely convinced there's not something more going on.
 For example, besides the IP referenced above, our 205.134.0.0 subnets 
 are also failing.  I pulled this trace from Above.net's looking glass:
 traceroute to 205.134.255.113 (205.134.255.113), 30 hops max, 40 byte 
 packets
   1  xe-0-1-0.mpr1.lax12.us.above.net (64.125.31.189)  22.612 ms  0.421 
 ms  0.368 ms
   2  10gigabitethernet1-3.core1.lax1.he.net (206.223.123.37)  0.944 ms 
 12.178 ms  0.956 ms
   3  * * *
   4  * * *
   5  * * *
   6  * * *
   7  * * *
   8  * * *
   9  * * *
 10  * * *
 11  * * *
 12  * * *
 13  * * *
 14  * * *
 15  * * *
 16  * * *
 17  * * *
 18  * * *
 19  * * *
 20  * * *
 21  * * *
 22  * * *
 23  * * *
 24  * * *
 25  * * *
 26  * * *
 27  * corporate-colocation-inc.gigabitethernet3-16.core1.lax1.he.net 
 (72.52.77.38)  1.135 ms !H *
 28  * * *
 29  * * *
 Tracing out from 205.134.255.113 anywhere yields this:
 root at wczverify1 [~]# traceroute above.net
 traceroute to above.net (64.125.228.35), 30 hops max, 40 byte packets
   1  74.124.198.1 (74.124.198.1)  0.461 ms  0.465 ms  0.530 ms
   2  gige-g2-19.core1.lax2.he.net (64.62.194.161)  0.363 ms  0.433 ms 
 0.509 ms
   3  * * *
   4  * * *
   5  * * *
   6  * * *
 If you could shed any additional light on this issue for us I'd be most 
 appreciative.
 Thanks.
 Best Regards,
 Erik Soroka
 Tier III System Administrator
 InMotion Hosting, Inc.
 eriks at inmotionhosting.com
 888-321-4678 (ext 834)
 757-416-6575 (int'l)
 On 04/26/2012 07:20 PM, Rob Mosher wrote:
 -- 
###############################################################
END
###############################################################

###############################################################
DNS issues with .gov on Tuesday 4/24?
###############################################################
 -----BEGIN PGP SIGNED MESSAGE-----
 Hash: SHA1
 Was anyone aware of any issues that might have affected DNS resolution for some .gov domains (specifically grants.gov) on Tuesday 4/24 around 3-5pm US Central Time?  We weren't alerted to the problem until 4:25pm, and things started working again around 4:45pm, so we didn't have time to really troubleshoot.  By the time we saw the ticket and the like, all we had time to do was verify that DNS was failing for us from AS103 and a couple of other DNS testing sites like kloth.net with the "no servers could be reached" error.  Users report that DNS resolution was working though from their cellular data networks as well as the public Wi-Fi in the Burger King across the street from their offices, so it does not appear to have been a universal outage. 
 Anecdotally, our users report that there may have been similar errors with nsf.gov and nih.gov earlier that day, but we have no actual data on that.  We also found some earlier issues noted with .gov such as the one mentioned at <http://forums.comcast.com/t5/Connectivity-and-Modem-Help/Re-Comcast-DNS-issue-gov-site-not-resolving/td-p/1261913>.  However, we have been unable to find any actual external verification of our exact issue, which would be useful since we had a few grant proposals get rejected because our researchers missed their deadline.
 Obviously there are any number of possible causes for this problem, and the grants.gov people seem to be having issues escalating our tickets with them to the group that would have a better chance of investigating from their end (and of course the actual problem could have been outside both of our spheres of influence).  
 Thanks for any information that can be shared!
 - -- 
 Julian Y. Koh
 Manager, Network Transport, Telecommunications and Network Services
 Northwestern University Information Technology (NUIT)
 2001 Sheridan Road #G-166
 Evanston, IL 60208
 847-467-5780
 NUIT Web Site: <http://www.it.northwestern.edu/>
 PGP Public Key:<http://bt.ittns.northwestern.edu/julian/pgppubkey.html>
 On Mon, Apr 30, 2012 at 12:00:27PM +0000,
  Julian Y Koh <kohster at northwestern.edu> wrote 
  a message of 37 lines which said:
 grants.gov has only two name servers and, from their IP addresses,
 they are probably next to each other. This is bad practice. Any
 routing issue and the domain is down.
 Badly managed domain, I would say.
###############################################################
END
###############################################################

